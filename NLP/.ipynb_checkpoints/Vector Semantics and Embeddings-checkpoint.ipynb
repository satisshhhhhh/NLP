{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e99256e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0231a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Embedding for 'cat': 0.5773502691896258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def preprocess(sentence):\n",
    "    # Tokenize the sentence\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Return the preprocessed sentence\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def get_word_embeddings(sentence, target_word):\n",
    "    # Preprocess the sentence\n",
    "    preprocessed_sentence = preprocess(sentence)\n",
    "    \n",
    "    # Create a vectorizer instance\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    # Fit and transform the preprocessed sentence\n",
    "    tfidf_matrix = vectorizer.fit_transform([preprocessed_sentence])\n",
    "    \n",
    "    # Get the feature names (words)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    # Get the index of the target word\n",
    "    target_word_index = feature_names.index(target_word)\n",
    "    \n",
    "    # Get the word embeddings\n",
    "    embeddings = tfidf_matrix.toarray()[0]\n",
    "    \n",
    "    # Get the embedding value for the target word\n",
    "    target_word_embedding = embeddings[target_word_index]\n",
    "    \n",
    "    return target_word_embedding\n",
    "\n",
    "# Example usage\n",
    "sentence = \"The cat is sitting on the mat\"\n",
    "target_word = \"cat\"\n",
    "\n",
    "# Get the word embeddings for the target word in the sentence\n",
    "target_word_embedding = get_word_embeddings(sentence, target_word)\n",
    "\n",
    "print(f\"Word Embedding for '{target_word}': {target_word_embedding}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c101b47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45171bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Embedding for 'cat': 0.5773502691896258\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def preprocess(sentence):\n",
    "    # Tokenize the sentence\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Return the preprocessed sentence\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def get_word_embeddings(sentence, target_word):\n",
    "    # Preprocess the sentence\n",
    "    preprocessed_sentence = preprocess(sentence)\n",
    "    \n",
    "    # Create a vectorizer instance\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    # Fit and transform the preprocessed sentence\n",
    "    tfidf_matrix = vectorizer.fit_transform([preprocessed_sentence])\n",
    "    \n",
    "    # Get the feature names (words)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    # Get the index of the target word\n",
    "    target_word_index = feature_names.index(target_word)\n",
    "    \n",
    "    # Get the word embeddings\n",
    "    embeddings = tfidf_matrix.toarray()[0]\n",
    "    \n",
    "    # Get the embedding value for the target word\n",
    "    target_word_embedding = embeddings[target_word_index]\n",
    "    \n",
    "    return target_word_embedding\n",
    "\n",
    "# Example usage\n",
    "sentence = \"The cat is sitting on the mat\"\n",
    "target_word = \"cat\"\n",
    "\n",
    "# Get the word embeddings for the target word in the sentence\n",
    "target_word_embedding = get_word_embeddings(sentence, target_word)\n",
    "\n",
    "print(f\"Word Embedding for '{target_word}': {target_word_embedding}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79387ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "567afc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Embedding for 'cat': 0.5773502691896258\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def lemmatize_word(token):\n",
    "    # Get the word's part of speech tag\n",
    "    pos_tag = nltk.pos_tag([token])[0][1][0].upper()\n",
    "    \n",
    "    # Map the POS tag to WordNet's POS tag format\n",
    "    if pos_tag.startswith('J'):\n",
    "        pos_tag = wordnet.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        pos_tag = wordnet.VERB\n",
    "    elif pos_tag.startswith('N'):\n",
    "        pos_tag = wordnet.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        pos_tag = wordnet.ADV\n",
    "    else:\n",
    "        pos_tag = wordnet.NOUN  # Default to noun if the POS tag is not recognized\n",
    "    \n",
    "    # Lemmatize the word using WordNet's lemmatizer\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    lemma = lemmatizer.lemmatize(token, pos_tag)\n",
    "    \n",
    "    return lemma\n",
    "\n",
    "def preprocess(sentence):\n",
    "    # Tokenize the sentence\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    \n",
    "    # Remove stopwords and lemmatize the tokens\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [lemmatize_word(token) for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Return the preprocessed sentence\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def get_word_embeddings(sentence, target_word):\n",
    "    # Preprocess the sentence\n",
    "    preprocessed_sentence = preprocess(sentence)\n",
    "    \n",
    "    # Create a vectorizer instance\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    # Fit and transform the preprocessed sentence\n",
    "    tfidf_matrix = vectorizer.fit_transform([preprocessed_sentence])\n",
    "    \n",
    "    # Get the feature names (words)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    # Get the index of the target word\n",
    "    target_word_index = feature_names.index(target_word)\n",
    "    \n",
    "    # Get the word embeddings\n",
    "    embeddings = tfidf_matrix.toarray()[0]\n",
    "    \n",
    "    # Get the embedding value for the target word\n",
    "    target_word_embedding = embeddings[target_word_index]\n",
    "    \n",
    "    return target_word_embedding\n",
    "\n",
    "# Example usage\n",
    "sentence = \"The cat is sitting on the mat\"\n",
    "target_word = \"cat\"\n",
    "\n",
    "# Get the word embeddings for the target word in the sentence\n",
    "target_word_embedding = get_word_embeddings(sentence, target_word)\n",
    "\n",
    "print(f\"Word Embedding for '{target_word}': {target_word_embedding}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c8d690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9762ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bc68237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_23600\\2228729307.py:57: RuntimeWarning: overflow encountered in multiply\n",
      "  context_embedding[noise_idx] -= learning_rate * (1 - similarity) * target_emb\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_23600\\2228729307.py:42: RuntimeWarning: overflow encountered in multiply\n",
      "  context_embedding[context_idx] -= learning_rate * similarity * target_emb\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'word2idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 78>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Access the embeddings for a target word\u001b[39;00m\n\u001b[0;32m     77\u001b[0m target_word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbooks\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 78\u001b[0m target_idx \u001b[38;5;241m=\u001b[39m \u001b[43mword2idx\u001b[49m[target_word]\n\u001b[0;32m     79\u001b[0m target_vector \u001b[38;5;241m=\u001b[39m target_embedding[target_idx]\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding for target word \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbooks\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, target_vector)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word2idx' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "import math\n",
    "\n",
    "def skipgram_negative_sampling(corpus, window_size, embedding_dim, num_negative_samples, learning_rate, epochs):\n",
    "    # Step 1: Preprocess the corpus and create vocabulary\n",
    "    words = [word for sentence in corpus for word in sentence]\n",
    "    word_counts = Counter(words)\n",
    "    vocabulary = list(word_counts.keys())\n",
    "    vocab_size = len(vocabulary)\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "    # Step 2: Initialize embedding matrices\n",
    "    target_embedding = np.random.uniform(-1, 1, (vocab_size, embedding_dim))\n",
    "    context_embedding = np.random.uniform(-1, 1, (vocab_size, embedding_dim))\n",
    "\n",
    "    # Step 3: Training the skip-gram model\n",
    "    for _ in range(epochs):\n",
    "        for sentence in corpus:\n",
    "            for i, target_word in enumerate(sentence):\n",
    "                target_idx = word2idx[target_word]\n",
    "\n",
    "                # Generate context words within the window\n",
    "                start = max(0, i - window_size)\n",
    "                end = min(i + window_size, len(sentence))\n",
    "                context_words = sentence[start:i] + sentence[i+1:end+1]\n",
    "\n",
    "                for context_word in context_words:\n",
    "                    context_idx = word2idx[context_word]\n",
    "\n",
    "                    # Update target and context embeddings\n",
    "                    target_emb = target_embedding[target_idx]\n",
    "                    context_emb = context_embedding[context_idx]\n",
    "\n",
    "                    # Calculate similarity between target and context embeddings\n",
    "                    similarity = np.dot(target_emb, context_emb)\n",
    "\n",
    "                    # Update target and context embeddings using gradient descent\n",
    "                    target_embedding[target_idx] -= learning_rate * similarity * context_emb\n",
    "                    context_embedding[context_idx] -= learning_rate * similarity * target_emb\n",
    "\n",
    "                # Negative sampling\n",
    "                for _ in range(num_negative_samples):\n",
    "                    noise_word = random.choice(vocabulary)\n",
    "                    noise_idx = word2idx[noise_word]\n",
    "\n",
    "                    # Update target and noise embeddings using gradient descent\n",
    "                    target_emb = target_embedding[target_idx]\n",
    "                    noise_emb = context_embedding[noise_idx]\n",
    "\n",
    "                    similarity = np.dot(target_emb, noise_emb)\n",
    "\n",
    "                    # Update target and noise embeddings using gradient descent with negative similarity\n",
    "                    target_embedding[target_idx] -= learning_rate * (1 - similarity) * noise_emb\n",
    "                    context_embedding[noise_idx] -= learning_rate * (1 - similarity) * target_emb\n",
    "\n",
    "    return target_embedding, context_embedding\n",
    "\n",
    "\n",
    "# Example usage\n",
    "corpus = [[\"I\", \"love\", \"to\", \"read\", \"books\"],\n",
    "          [\"Reading\", \"is\", \"my\", \"favorite\", \"hobby\"],\n",
    "          [\"Books\", \"expand\", \"my\", \"knowledge\"]]\n",
    "\n",
    "window_size = 2\n",
    "embedding_dim = 100\n",
    "num_negative_samples = 5\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "target_embedding, context_embedding = skipgram_negative_sampling(corpus, window_size, embedding_dim,\n",
    "                                                                 num_negative_samples, learning_rate, epochs)\n",
    "\n",
    "# Access the embeddings for a target word\n",
    "target_word = \"books\"\n",
    "target_idx = word2idx[target_word]\n",
    "target_vector = target_embedding[target_idx]\n",
    "print(\"Embedding for target word 'books':\", target_vector)\n",
    "\n",
    "# Access the embeddings for a context word\n",
    "context_word = \"reading\"\n",
    "context_idx = word2idx[context_word]\n",
    "context_vector = context_embedding[context_idx]\n",
    "print(\"Embedding for context word 'reading':\", context_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c51e5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'i'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 82>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     79\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[0;32m     80\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m---> 82\u001b[0m target_embedding, context_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mskipgram_negative_sampling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m                                                                 \u001b[49m\u001b[43mnum_negative_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Access the embeddings for a target word\u001b[39;00m\n\u001b[0;32m     86\u001b[0m target_word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbooks\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36mskipgram_negative_sampling\u001b[1;34m(corpus, window_size, embedding_dim, num_negative_samples, learning_rate, epochs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m corpus:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, target_word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sentence):\n\u001b[1;32m---> 32\u001b[0m         target_idx \u001b[38;5;241m=\u001b[39m \u001b[43mword2idx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_word\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;66;03m# Generate context words within the window\u001b[39;00m\n\u001b[0;32m     35\u001b[0m         start \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, i \u001b[38;5;241m-\u001b[39m window_size)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'i'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "import math\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def skipgram_negative_sampling(corpus, window_size, embedding_dim, num_negative_samples, learning_rate, epochs):\n",
    "    # Step 1: Preprocess the corpus and create vocabulary\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuations = set(string.punctuation)\n",
    "    words = [word.lower() for sentence in corpus for word in sentence if word.lower() not in stop_words and word.lower() not in punctuations]\n",
    "    word_counts = Counter(words)\n",
    "    vocabulary = list(word_counts.keys())\n",
    "    vocab_size = len(vocabulary)\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "    # Step 2: Initialize embedding matrices\n",
    "    target_embedding = np.random.uniform(-1, 1, (vocab_size, embedding_dim))\n",
    "    context_embedding = np.random.uniform(-1, 1, (vocab_size, embedding_dim))\n",
    "\n",
    "    # Step 3: Training the skip-gram model\n",
    "    for _ in range(epochs):\n",
    "        for sentence in corpus:\n",
    "            for i, target_word in enumerate(sentence):\n",
    "                target_idx = word2idx[target_word]\n",
    "\n",
    "                # Generate context words within the window\n",
    "                start = max(0, i - window_size)\n",
    "                end = min(i + window_size, len(sentence))\n",
    "                context_words = sentence[start:i] + sentence[i+1:end+1]\n",
    "\n",
    "                for context_word in context_words:\n",
    "                    context_idx = word2idx[context_word]\n",
    "\n",
    "                    # Update target and context embeddings\n",
    "                    target_emb = target_embedding[target_idx]\n",
    "                    context_emb = context_embedding[context_idx]\n",
    "\n",
    "                    # Calculate similarity between target and context embeddings\n",
    "                    similarity = np.dot(target_emb, context_emb)\n",
    "\n",
    "                    # Update target and context embeddings using gradient descent\n",
    "                    target_embedding[target_idx] -= learning_rate * similarity * context_emb\n",
    "                    context_embedding[context_idx] -= learning_rate * similarity * target_emb\n",
    "\n",
    "                # Negative sampling\n",
    "                for _ in range(num_negative_samples):\n",
    "                    noise_word = random.choice(vocabulary)\n",
    "                    noise_idx = word2idx[noise_word]\n",
    "\n",
    "                    # Update target and noise embeddings using gradient descent\n",
    "                    target_emb = target_embedding[target_idx]\n",
    "                    noise_emb = context_embedding[noise_idx]\n",
    "\n",
    "                    similarity = np.dot(target_emb, noise_emb)\n",
    "\n",
    "                    # Update target and noise embeddings using gradient descent with negative similarity\n",
    "                    target_embedding[target_idx] -= learning_rate * (1 - similarity) * noise_emb\n",
    "                    context_embedding[noise_idx] -= learning_rate * (1 - similarity) * target_emb\n",
    "\n",
    "    return target_embedding, context_embedding\n",
    "\n",
    "\n",
    "# Example usage\n",
    "corpus = [[\"i\", \"love\", \"to\", \"read\", \"books\"],\n",
    "          [\"Reading\", \"is\", \"my\", \"favorite\", \"hobby\"],\n",
    "          [\"Books\", \"expand\", \"my\", \"knowledge\"]]\n",
    "\n",
    "window_size = 2\n",
    "embedding_dim = 100\n",
    "num_negative_samples = 5\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "target_embedding, context_embedding = skipgram_negative_sampling(corpus, window_size, embedding_dim,\n",
    "                                                                 num_negative_samples, learning_rate, epochs)\n",
    "\n",
    "# Access the embeddings for a target word\n",
    "target_word = \"books\"\n",
    "word2idx = {word: idx for idx, word in enumerate(vocabulary)}  # Create word2idx using the vocabulary within the function\n",
    "target_idx = word2idx[target_word]\n",
    "target_vector = target_embedding[target_idx]\n",
    "print(\"Embedding for target word 'books':\", target_vector)\n",
    "\n",
    "# Access the embeddings for a context word\n",
    "context_word = \"reading\"\n",
    "context_idx = word2idx[context_word]\n",
    "context_vector = context_embedding[context_idx]\n",
    "print(\"Embedding for context word 'reading':\", context_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03b3b19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec47dcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 Loss: -25.8659\n",
      "Epoch: 200 Loss: -25.7636\n",
      "Epoch: 300 Loss: -25.7524\n",
      "Epoch: 400 Loss: -25.7512\n",
      "Epoch: 500 Loss: -25.7510\n",
      "Epoch: 600 Loss: -25.7510\n",
      "Epoch: 700 Loss: -25.7510\n",
      "Epoch: 800 Loss: -25.7510\n",
      "Epoch: 900 Loss: -25.7510\n",
      "Epoch: 1000 Loss: -25.7510\n",
      "football: [ 1.58880190e-09  5.56114850e-10 -5.46111448e-11 -1.03751929e-09\n",
      " -9.75237605e-12]\n",
      "to: [-4.29867850e-10 -3.01787997e-10  2.53605885e-10  7.61082327e-10\n",
      " -7.32855206e-10]\n",
      "play: [-8.18868096e-11 -2.18262106e-11  2.43801340e-10 -4.22619308e-10\n",
      " -9.59988836e-11]\n",
      "with: [-1.07356095e-09 -2.29034415e-10 -4.39095631e-10  6.76358087e-10\n",
      "  8.45706406e-10]\n",
      "my: [-0.99138595 -0.77163897 -0.83172242 -0.21786612  0.26191911]\n",
      "love: [-0.92485305 -0.4197433   0.83177126  0.01199458  0.85986134]\n",
      "friends: [ 0.64663647 -0.56129889  0.63498935  0.31828337 -0.69871042]\n",
      "I: [ 0.837873   -0.17149479  0.46193055 -0.39327006  0.05000026]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from math import exp, log\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    'I',\n",
    "    'love',\n",
    "    'to',\n",
    "    'play',\n",
    "    'football',\n",
    "    'with',\n",
    "    'my',\n",
    "    'friends'\n",
    "]\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Initialize word embeddings with random values\n",
    "word_embeddings = defaultdict(lambda: np.random.uniform(-1, 1, 5))\n",
    "\n",
    "# Define hyperparameters\n",
    "window_size = 2\n",
    "embedding_size = 5\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000\n",
    "\n",
    "# Generate training data\n",
    "training_data = []\n",
    "for i in range(window_size, len(corpus) - window_size):\n",
    "    center_word = corpus[i]\n",
    "    context_words = corpus[i - window_size:i] + corpus[i + 1:i + window_size + 1]\n",
    "    training_data.append((center_word, context_words))\n",
    "\n",
    "# Generate noise word sampling table\n",
    "word_freq = defaultdict(int)\n",
    "for center_word, _ in training_data:\n",
    "    word_freq[center_word] += 1\n",
    "\n",
    "total_words = sum(word_freq.values())\n",
    "word_sampling_table = []\n",
    "for center_word in word_freq.keys():\n",
    "    word_prob = word_freq[center_word] / total_words\n",
    "    word_sampling_table.extend([center_word] * int(word_prob * 1e6))\n",
    "\n",
    "# Logistic function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    random.shuffle(training_data)\n",
    "\n",
    "    for center_word, context_words in training_data:\n",
    "        center_embedding = word_embeddings[center_word]\n",
    "\n",
    "        # Update embeddings for context words\n",
    "        for context_word in context_words:\n",
    "            context_embedding = word_embeddings[context_word]\n",
    "\n",
    "            # Compute loss function\n",
    "            noise_words = random.sample(word_sampling_table, k=5)  # Select 5 noise words\n",
    "            noise_prob = sum(sigmoid(np.dot(center_embedding, word_embeddings[noise_word])) for noise_word in noise_words)\n",
    "            context_prob = sigmoid(np.dot(center_embedding, context_embedding))\n",
    "            loss = log(context_prob / noise_prob)\n",
    "\n",
    "            # Update word embeddings using stochastic gradient descent\n",
    "            gradient = (1 - context_prob) * center_embedding + sum(context_prob * word_embeddings[noise_word] for noise_word in noise_words)\n",
    "            word_embeddings[center_word] -= learning_rate * gradient\n",
    "\n",
    "            total_loss += loss\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch: {epoch + 1} Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Print word embeddings\n",
    "for word, embedding in word_embeddings.items():\n",
    "    print(f\"{word}: {embedding}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61b5e1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 Loss: -25.7516\n",
      "Epoch: 200 Loss: -25.7440\n",
      "Epoch: 300 Loss: -25.7496\n",
      "Epoch: 400 Loss: -25.7508\n",
      "Epoch: 500 Loss: -25.7510\n",
      "Epoch: 600 Loss: -25.7510\n",
      "Epoch: 700 Loss: -25.7510\n",
      "Epoch: 800 Loss: -25.7510\n",
      "Epoch: 900 Loss: -25.7510\n",
      "Epoch: 1000 Loss: -25.7510\n",
      "Word Embeddings:\n",
      "football: 0.0000 0.0000 -0.0000 -0.0000 -0.0000\n",
      "to: -0.0000 -0.0000 0.0000 0.0000 -0.0000\n",
      "play: -0.0000 -0.0000 0.0000 0.0000 0.0000\n",
      "with: -0.0000 -0.0000 0.0000 -0.0000 0.0000\n",
      "my: -0.5145 0.1089 0.4041 -0.4982 0.2397\n",
      "love: -0.7791 0.1602 0.4451 0.0216 -0.8995\n",
      "friends: -0.5265 0.1321 0.2581 -0.2517 0.7662\n",
      "I: -0.2670 0.1070 -0.2035 -0.4504 0.6840\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from math import exp, log\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    'I',\n",
    "    'love',\n",
    "    'to',\n",
    "    'play',\n",
    "    'football',\n",
    "    'with',\n",
    "    'my',\n",
    "    'friends'\n",
    "]\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Initialize word embeddings with random values\n",
    "word_embeddings = defaultdict(lambda: np.random.uniform(-1, 1, 5))\n",
    "\n",
    "# Define hyperparameters\n",
    "window_size = 2\n",
    "embedding_size = 5\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000\n",
    "\n",
    "# Generate training data\n",
    "training_data = []\n",
    "for i in range(window_size, len(corpus) - window_size):\n",
    "    center_word = corpus[i]\n",
    "    context_words = corpus[i - window_size:i] + corpus[i + 1:i + window_size + 1]\n",
    "    training_data.append((center_word, context_words))\n",
    "\n",
    "# Generate noise word sampling table\n",
    "word_freq = defaultdict(int)\n",
    "for center_word, _ in training_data:\n",
    "    word_freq[center_word] += 1\n",
    "\n",
    "total_words = sum(word_freq.values())\n",
    "word_sampling_table = []\n",
    "for center_word in word_freq.keys():\n",
    "    word_prob = word_freq[center_word] / total_words\n",
    "    word_sampling_table.extend([center_word] * int(word_prob * 1e6))\n",
    "\n",
    "# Logistic function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    random.shuffle(training_data)\n",
    "\n",
    "    for center_word, context_words in training_data:\n",
    "        center_embedding = word_embeddings[center_word]\n",
    "\n",
    "        # Update embeddings for context words\n",
    "        for context_word in context_words:\n",
    "            context_embedding = word_embeddings[context_word]\n",
    "\n",
    "            # Compute loss function\n",
    "            noise_words = random.sample(word_sampling_table, k=5)  # Select 5 noise words\n",
    "            noise_prob = sum(sigmoid(np.dot(center_embedding, word_embeddings[noise_word])) for noise_word in noise_words)\n",
    "            context_prob = sigmoid(np.dot(center_embedding, context_embedding))\n",
    "            loss = log(context_prob / noise_prob)\n",
    "\n",
    "            # Update word embeddings using stochastic gradient descent\n",
    "            gradient = (1 - context_prob) * center_embedding + sum(context_prob * word_embeddings[noise_word] for noise_word in noise_words)\n",
    "            word_embeddings[center_word] -= learning_rate * gradient\n",
    "\n",
    "            total_loss += loss\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch: {epoch + 1} Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Print word embeddings\n",
    "print(\"Word Embeddings:\")\n",
    "for word, embedding in word_embeddings.items():\n",
    "    embedding_str = ' '.join(f'{val:.4f}' for val in embedding)\n",
    "    print(f\"{word}: {embedding_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1d55231",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 59>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Compute loss function\u001b[39;00m\n\u001b[0;32m     71\u001b[0m noise_words \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(word_sampling_table, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)  \u001b[38;5;66;03m# Select 5 noise words\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m noise_embeddings \u001b[38;5;241m=\u001b[39m [noise_embeddings[noise_word] \u001b[38;5;28;01mfor\u001b[39;00m noise_word \u001b[38;5;129;01min\u001b[39;00m noise_words]\n\u001b[0;32m     73\u001b[0m noise_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(sigmoid(np\u001b[38;5;241m.\u001b[39mdot(target_embedding, noise_embedding)) \u001b[38;5;28;01mfor\u001b[39;00m noise_embedding \u001b[38;5;129;01min\u001b[39;00m noise_embeddings)\n\u001b[0;32m     74\u001b[0m context_prob \u001b[38;5;241m=\u001b[39m sigmoid(np\u001b[38;5;241m.\u001b[39mdot(target_embedding, context_embedding))\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Compute loss function\u001b[39;00m\n\u001b[0;32m     71\u001b[0m noise_words \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(word_sampling_table, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)  \u001b[38;5;66;03m# Select 5 noise words\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m noise_embeddings \u001b[38;5;241m=\u001b[39m [\u001b[43mnoise_embeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnoise_word\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m noise_word \u001b[38;5;129;01min\u001b[39;00m noise_words]\n\u001b[0;32m     73\u001b[0m noise_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(sigmoid(np\u001b[38;5;241m.\u001b[39mdot(target_embedding, noise_embedding)) \u001b[38;5;28;01mfor\u001b[39;00m noise_embedding \u001b[38;5;129;01min\u001b[39;00m noise_embeddings)\n\u001b[0;32m     74\u001b[0m context_prob \u001b[38;5;241m=\u001b[39m sigmoid(np\u001b[38;5;241m.\u001b[39mdot(target_embedding, context_embedding))\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from math import exp, log\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    'I',\n",
    "    'love',\n",
    "    'to',\n",
    "    'play',\n",
    "    'football',\n",
    "    'with',\n",
    "    'my',\n",
    "    'friends'\n",
    "]\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Initialize word embeddings with random values\n",
    "word_embeddings = defaultdict(lambda: np.random.uniform(-1, 1, 5))\n",
    "\n",
    "# Define hyperparameters\n",
    "window_size = 2\n",
    "embedding_size = 5\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000\n",
    "\n",
    "# Generate training data\n",
    "training_data = []\n",
    "for i in range(window_size, len(corpus) - window_size):\n",
    "    center_word = corpus[i]\n",
    "    context_words = corpus[i - window_size:i] + corpus[i + 1:i + window_size + 1]\n",
    "    training_data.append((center_word, context_words))\n",
    "\n",
    "# Generate noise word sampling table\n",
    "word_freq = defaultdict(int)\n",
    "for center_word, _ in training_data:\n",
    "    word_freq[center_word] += 1\n",
    "\n",
    "total_words = sum(word_freq.values())\n",
    "word_sampling_table = []\n",
    "for center_word in word_freq.keys():\n",
    "    word_prob = word_freq[center_word] / total_words\n",
    "    word_sampling_table.extend([center_word] * int(word_prob * 1e6))\n",
    "\n",
    "# Logistic function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    random.shuffle(training_data)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}:\")\n",
    "    for center_word, context_words in training_data:\n",
    "        center_embedding = word_embeddings[center_word]\n",
    "\n",
    "        # Update embeddings for context words\n",
    "        for context_word in context_words:\n",
    "            context_embedding = word_embeddings[context_word]\n",
    "\n",
    "            # Compute loss function\n",
    "            noise_words = random.sample(word_sampling_table, k=5)  # Select 5 noise words\n",
    "            noise_prob = sum(sigmoid(np.dot(center_embedding, word_embeddings[noise_word])) for noise_word in noise_words)\n",
    "            context_prob = sigmoid(np.dot(center_embedding, context_embedding))\n",
    "            loss = log(context_prob / noise_prob)\n",
    "\n",
    "            # Update word embeddings using stochastic gradient descent\n",
    "            gradient = (1 - context_prob) * center_embedding + sum(context_prob * word_embeddings[noise_word] for noise_word in noise_words)\n",
    "            word_embeddings[center_word] -= learning_rate * gradient\n",
    "\n",
    "            total_loss += loss\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"  Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Print word embeddings\n",
    "print(\"Word Embeddings:\")\n",
    "for word, embedding in word_embeddings.items():\n",
    "    embedding_str = ' '.join(f'{val:.4f}' for val in embedding)\n",
    "    print(f\"{word}: {embedding_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71a34928",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 Loss: -19.4994\n",
      "Epoch: 200 Loss: -19.8293\n",
      "Epoch: 300 Loss: -17.9225\n",
      "Epoch: 400 Loss: -18.4023\n",
      "Epoch: 500 Loss: -19.2458\n",
      "Epoch: 600 Loss: -20.7343\n",
      "Epoch: 700 Loss: -18.9494\n",
      "Epoch: 800 Loss: -18.6678\n",
      "Epoch: 900 Loss: -20.5577\n",
      "Epoch: 1000 Loss: -20.9200\n",
      "Target Word Embeddings:\n",
      "football: 0.5141 -0.5435 -0.1455 2.4211 -0.6335\n",
      "play: 0.3464 -0.2436 -0.1005 1.4191 -0.4414\n",
      "with: 0.5737 -0.5057 -0.0869 2.5148 -0.7381\n",
      "to: 0.5348 -0.4959 -0.1736 2.5381 -0.7130\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from math import exp, log\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    'I',\n",
    "    'love',\n",
    "    'to',\n",
    "    'play',\n",
    "    'football',\n",
    "    'with',\n",
    "    'my',\n",
    "    'friends'\n",
    "]\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Initialize word embeddings with random values\n",
    "word_embeddings = defaultdict(lambda: np.random.uniform(-1, 1, 5))\n",
    "\n",
    "# Define hyperparameters\n",
    "window_size = 2\n",
    "embedding_size = 5\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000\n",
    "\n",
    "# Generate training data\n",
    "training_data = []\n",
    "for i in range(window_size, len(corpus) - window_size):\n",
    "    center_word = corpus[i]\n",
    "    context_words = corpus[i - window_size:i] + corpus[i + 1:i + window_size + 1]\n",
    "    training_data.append((center_word, context_words))\n",
    "\n",
    "# Generate noise word sampling table\n",
    "word_freq = defaultdict(int)\n",
    "for center_word, _ in training_data:\n",
    "    word_freq[center_word] += 1\n",
    "\n",
    "total_words = sum(word_freq.values())\n",
    "word_sampling_table = []\n",
    "for center_word in word_freq.keys():\n",
    "    word_prob = word_freq[center_word] / total_words\n",
    "    word_sampling_table.extend([center_word] * int(word_prob * 1e6))\n",
    "\n",
    "# Generate embeddings for all target words\n",
    "target_embeddings = defaultdict(lambda: np.random.uniform(-1, 1, embedding_size))\n",
    "# Generate embeddings for all context words and noise words\n",
    "context_embeddings = defaultdict(lambda: np.random.uniform(-1, 1, embedding_size))\n",
    "noise_embeddings = {}\n",
    "\n",
    "# Logistic function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    random.shuffle(training_data)\n",
    "\n",
    "    for center_word, context_words in training_data:\n",
    "        target_embedding = target_embeddings[center_word]\n",
    "\n",
    "        # Update embeddings for context words\n",
    "        for context_word in context_words:\n",
    "            context_embedding = context_embeddings[context_word]\n",
    "\n",
    "            # Compute loss function\n",
    "            noise_words = random.sample(word_sampling_table, k=5)  # Select 5 noise words\n",
    "            noise_embeddings = {noise_word: word_embeddings[noise_word] for noise_word in noise_words}\n",
    "            noise_prob = sum(sigmoid(np.dot(target_embedding, noise_embeddings[noise_word])) for noise_word in noise_words)\n",
    "            context_prob = sigmoid(np.dot(target_embedding, context_embedding))\n",
    "            loss = log(context_prob / noise_prob)\n",
    "\n",
    "            # Update target word embeddings using stochastic gradient descent\n",
    "            gradient = (1 - context_prob) * target_embedding + sum(context_prob * noise_embeddings[noise_word] for noise_word in noise_words)\n",
    "            target_embeddings[center_word] -= learning_rate * gradient\n",
    "\n",
    "            total_loss += loss\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch: {epoch + 1} Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Print target word embeddings\n",
    "print(\"Target Word Embeddings:\")\n",
    "for word, embedding in target_embeddings.items():\n",
    "    embedding_str = ' '.join(f'{val:.4f}' for val in embedding)\n",
    "    print(f\"{word}: {embedding_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b8399f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
