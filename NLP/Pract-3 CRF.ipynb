{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6654740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import string\n",
    "from tabulate import tabulate\n",
    "\n",
    "LABEL_TYPES = {'company', 'vbd', 'person', 'cc', 'date', 'in', 'amount', 'duration'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3b923c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    cleaned_text = ''.join([word for word in text if word not in string.punctuation])\n",
    "    tokens = cleaned_text.split(\" \")\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abe23c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d37f76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_functions(label_types):\n",
    "    feature_functions = [\n",
    "        lambda position, label1, label2: int(position == 0),\n",
    "    ]\n",
    "\n",
    "    for label1 in label_types:\n",
    "        for label2 in label_types:\n",
    "            feature_functions.append(lambda position, l1=label1, l2=label2: int(text_train_tags.get(text_train[max(0, position - 1)], '') == l1 and text_train_tags.get(text_train[position], '') == l2))\n",
    "\n",
    "    return feature_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbcbaaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = \"Google hired XYZ in 2005, for $100,000 per annum.\"\n",
    "text_test = \"Apple hired ABC in 1995, for $10 per hour.\"\n",
    "\n",
    "text_train = preprocess_text(text_train)\n",
    "text_test = preprocess_text(text_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd04688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train_tags = {\n",
    "    'Google': 'company',\n",
    "    'hired': 'vbd',\n",
    "    'XYZ': 'person',\n",
    "    'in': 'cc',\n",
    "    '2005': 'date',\n",
    "    'for': 'in',\n",
    "    '100000': 'amount',\n",
    "    'per': 'in',\n",
    "    'annum': 'duration'\n",
    "}\n",
    "\n",
    "\n",
    "feature_func = generate_feature_functions(LABEL_TYPES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffd0b656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Updated weights:  [0.01657684 0.00768371 0.02303112 0.0091025  0.02000166 0.008214\n",
      " 0.01742058 0.01599503 0.00966755 0.02145727 0.01273574 0.0147577\n",
      " 0.0276122  0.01566099 0.00727098 0.01324132 0.02278519 0.01477745\n",
      " 0.02291874 0.00902008 0.00952149 0.02070504 0.00971179 0.00700216\n",
      " 0.01653353 0.00766292 0.02481472 0.00783717 0.01338783 0.02162266\n",
      " 0.00699731 0.02203066 0.01086969 0.02088893 0.01231717 0.01420193\n",
      " 0.01066792 0.01668393 0.01999609 0.01798903 0.01609942 0.01304213\n",
      " 0.02189315 0.00914234 0.00687407 0.01320614 0.0242446  0.0264546\n",
      " 0.00685645 0.00827969 0.01243399 0.01933687 0.00677793 0.01952646\n",
      " 0.02851719 0.01260005 0.00739855 0.00928296 0.01692183 0.02958445\n",
      " 0.0101412  0.02863214 0.01021034 0.0215486  0.01962223]\n",
      "Epoch 1 Updated weights:  [0.01024527 0.00490536 0.0269313  0.00638017 0.02163912 0.00544044\n",
      " 0.01746445 0.01529802 0.007005   0.02413061 0.01074273 0.01350179\n",
      " 0.03568453 0.01480526 0.0045027  0.01141151 0.02648649 0.01352982\n",
      " 0.02672771 0.00629078 0.00684151 0.02283103 0.0070548  0.00424709\n",
      " 0.01610439 0.00488478 0.0302351  0.00505818 0.01160798 0.02441978\n",
      " 0.00424253 0.0251383  0.0084018  0.02314636 0.01019999 0.01272121\n",
      " 0.0081611  0.01633222 0.02162979 0.01835644 0.01545319 0.01114631\n",
      " 0.0248953  0.00642355 0.00412718 0.01136451 0.0291643  0.03339072\n",
      " 0.00411078 0.00550808 0.01035046 0.02053368 0.00403797 0.02084683\n",
      " 0.03751522 0.0105657  0.00462584 0.00657746 0.01669492 0.03971565\n",
      " 0.00754457 0.03775007 0.0076245  0.02429013 0.02100566]\n",
      "Epoch 2 Updated weights:  [0.00435636 0.00224398 0.03150366 0.00337382 0.02243663 0.00263495\n",
      " 0.01608987 0.01310149 0.00390003 0.02656928 0.00757111 0.0107937\n",
      " 0.04874932 0.01245266 0.00196478 0.00831475 0.03070014 0.01082849\n",
      " 0.03113498 0.00330077 0.00375974 0.02438268 0.00394312 0.00179448\n",
      " 0.01418826 0.0022294  0.03769829 0.00235336 0.00853787 0.02706484\n",
      " 0.00179149 0.02831022 0.00517092 0.02490709 0.00698605 0.00984122\n",
      " 0.00494292 0.01450087 0.02242161 0.01738253 0.01330822 0.00801691\n",
      " 0.02788681 0.00340947 0.0017165  0.00826167 0.03564743 0.04397503\n",
      " 0.00170593 0.00268595 0.00714657 0.02068375 0.00165929 0.02117515\n",
      " 0.05268355 0.00737844 0.00204876 0.00353704 0.01500348 0.05755418\n",
      " 0.00437584 0.05319608 0.00444798 0.02684226 0.02142595]\n",
      "Epoch 3 Updated weights:  [0.00097376 0.00057545 0.03466651 0.0010833  0.02047571 0.00073827\n",
      " 0.01222417 0.00888774 0.00135644 0.02661589 0.00379599 0.00658032\n",
      " 0.06824228 0.00821431 0.00046826 0.00438983 0.03330452 0.00661324\n",
      " 0.03403917 0.00104714 0.0012815  0.02329587 0.00137976 0.00040682\n",
      " 0.01005728 0.00056966 0.04579865 0.00061954 0.00457393 0.02738996\n",
      " 0.00040577 0.0293698  0.00210106 0.02407774 0.00335074 0.00570171\n",
      " 0.0019591  0.01040312 0.02045445 0.01378106 0.00910625 0.00414831\n",
      " 0.02869118 0.00110111 0.00037972 0.00434644 0.0419919  0.0581584\n",
      " 0.0003761  0.00076056 0.00347093 0.01804817 0.00036027 0.01871769\n",
      " 0.07697389 0.00364719 0.00049967 0.00116568 0.01096782 0.08829117\n",
      " 0.00162165 0.07813869 0.00166331 0.02704131 0.01906274]\n",
      "Epoch 4 Updated weights:  [0.00007534 0.0000565  0.03260297 0.00015075 0.014405   0.00008316\n",
      " 0.0064712  0.00394674 0.00021367 0.02163777 0.00105457 0.00247583\n",
      " 0.09323312 0.00349262 0.00004104 0.00132129 0.03063751 0.00249507\n",
      " 0.03169227 0.00014302 0.00019564 0.01759745 0.0002194  0.00003299\n",
      " 0.0047811  0.00005562 0.05022024 0.00006336 0.00140824 0.0226218\n",
      " 0.00003286 0.02520852 0.00042127 0.01852211 0.00086899 0.00198227\n",
      " 0.00037794 0.00503855 0.01438181 0.00779386 0.00409828 0.00121024\n",
      " 0.02431069 0.00015461 0.00002965 0.00130109 0.04389486 0.07275188\n",
      " 0.00002921 0.00008709 0.00091783 0.01184381 0.00002732 0.01253234\n",
      " 0.11237998 0.00099114 0.00004539 0.0001689  0.00546914 0.1390297\n",
      " 0.00028188 0.11502913 0.00029319 0.02217666 0.01289255]\n"
     ]
    }
   ],
   "source": [
    "weights = np.random.rand(len(feature_func))\n",
    "np.set_printoptions(suppress=True)\n",
    "epochs = 5\n",
    "learning_rate = 0.05\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for token_pos in range(len(text_train)):\n",
    "        if text_train[token_pos] in text_train_tags:\n",
    "            features = [f(token_pos, text_train_tags.get(text_train[max(0, token_pos - 1)], ''), text_train_tags.get(text_train[token_pos], '')) for f in feature_func]\n",
    "            weights += learning_rate * np.multiply(weights, features)\n",
    "\n",
    "    print(f\"Epoch {epoch} Updated weights: \",softmax(weights))\n",
    "\n",
    "weights = preprocessing.normalize([weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61b34107",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = list(LABEL_TYPES)\n",
    "pred_dict = {}\n",
    "prev = 'company'\n",
    "\n",
    "for token_pos in range(1, len(text_test)):\n",
    "    probabilities = []\n",
    "    for tag in tags:\n",
    "        features = [f(token_pos, prev, tag) for f in feature_func]\n",
    "        weighted_features = np.multiply(weights, features)\n",
    "        probability = sum(weighted_features[0])\n",
    "        probabilities.append(probability)\n",
    "\n",
    "    probabilities = softmax(probabilities)\n",
    "    prev = tags[np.argmax(probabilities)]\n",
    "    pred_dict[text_test[token_pos]] = [round(prob, 3) for prob in probabilities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4417b82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_data = []\n",
    "for token, probs in pred_dict.items():\n",
    "    table_data.append([token] + probs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71eb6346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token      person    date     in    duration    amount     cc    vbd    company\n",
      "hired       0.001   0.001  0.001       0.001     0.001  0.001  0.993      0.001\n",
      "ABC         0.993   0.001  0.001       0.001     0.001  0.001  0.001      0.001\n",
      "in          0.001   0.001  0.001       0.001     0.001  0.993  0.001      0.001\n",
      "1995        0.001   0.993  0.001       0.001     0.001  0.001  0.001      0.001\n",
      "for         0.001   0.001  0.993       0.001     0.001  0.001  0.001      0.001\n",
      "10          0.001   0.001  0.001       0.001     0.993  0.001  0.001      0.001\n",
      "per         0.001   0.001  0.993       0.001     0.001  0.001  0.001      0.001\n",
      "hour        0.001   0.001  0.001       0.993     0.001  0.001  0.001      0.001\n"
     ]
    }
   ],
   "source": [
    "headers = [\"Token\"] + tags\n",
    "table = tabulate(table_data, headers=headers, tablefmt=\"plain\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8783968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e984cecb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da50d60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c04ff4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ae59f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7275f520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import string\n",
    "from tabulate import tabulate\n",
    "\n",
    "LABEL_TYPES = {'company', 'vbd', 'person', 'cc', 'date', 'in', 'amount', 'duration'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "59c52931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data : ['Google', 'hired', 'XYZ', 'in', '2005', 'for', '100000', 'per', 'annum']\n",
      "Test data : ['Apple', 'hired', 'ABC', 'in', '1995', 'for', '10', 'per', 'hour']\n"
     ]
    }
   ],
   "source": [
    "text_train=\"Google hired XYZ in 2005, for $100,000 per annum.\"\n",
    "text_train=''.join([word for word in text_train if word not in string.punctuation]).split(\" \")\n",
    "\n",
    "text_train_tags = {\n",
    "    'Google' : 'company',\n",
    "    'hired'  : 'vbd',\n",
    "    'XYZ'    : 'person',\n",
    "    'in'     : 'cc',\n",
    "    '2005'   : 'date',\n",
    "    'for'    : 'in',\n",
    "    '100000' : 'amount',\n",
    "    'per'    : 'in',\n",
    "    'annum'  : 'duration'\n",
    "}\n",
    "text_test = 'Apple hired ABC in 1995, for $10 per hour.'\n",
    "text_test=''.join([word for word in text_test if word not in string.punctuation]).split(\" \")\n",
    "\n",
    "print(f\"Train data : {text_train}\\nTest data : {text_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c990762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def softmax(x):\n",
    "    e_x =np.exp(x-np.max(x))\n",
    "    return np.round(e_x / e_x.sum(axis=0),3)\n",
    "\n",
    "\n",
    "feature_func =[\n",
    "    lambda position , label1, label2 : 1 if position == 0 else 0,\n",
    "    lambda position , label1, label2 : 1 if label1   == 'company' and label2 == 'vbd'      else 0,\n",
    "    lambda position , label1, label2 : 1 if label1   == 'vbd'     and label2 == 'person'   else 0,\n",
    "    lambda position , label1, label2 : 1 if label1   == 'person'  and label2 == 'cc'       else 0,\n",
    "    lambda position , label1, label2 : 1 if label1   == 'cc'      and label2 == 'date'     else 0,\n",
    "    lambda position , label1, label2 : 1 if label1   == 'date'    and label2 == 'in'       else 0,\n",
    "    lambda position , label1, label2 : 1 if label1   == 'in'      and label2 == 'amount'   else 0,\n",
    "    lambda position , label1, label2 : 1 if label1   == 'amount'  and label2 == 'in'       else 0,\n",
    "    lambda position , label1, label2 : 1 if label1   == 'in'      and label2 == ' '        else 0\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eaa89e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate transition scores using feature functions and weights\n",
    "def calculate_transition_scores(position, prev_label, current_label, feature_functions, weights):\n",
    "    features = [f(position, prev_label, current_label) for f in feature_functions]\n",
    "    return np.dot(weights, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "994ab920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights with random values\n",
    "weights = np.random.rand(len(feature_func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2593a686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 5\n",
    "learning_rate = 0.05\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for token_pos in range(len(text_train)):\n",
    "        if text_train[token_pos] in text_train_tags:\n",
    "            for current_label in LABEL_TYPES:\n",
    "                transition_scores = [\n",
    "                    calculate_transition_scores(token_pos, text_train_tags.get(text_train[max(0, token_pos - 1)], ''), current_label, feature_func, weights)\n",
    "                    for _ in LABEL_TYPES\n",
    "                ]\n",
    "                softmax_scores = softmax(transition_scores)\n",
    "                for i, label in enumerate(LABEL_TYPES):\n",
    "                    features = [f(token_pos, text_train_tags.get(text_train[max(0, token_pos - 1)], ''), current_label) for f in feature_func]\n",
    "                    weights += learning_rate * (int(current_label == label) - softmax_scores[i]) * np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9d21b06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize weights\n",
    "normalized_weights = preprocessing.normalize([weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee3b30a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on test data\n",
    "tags = list(LABEL_TYPES)\n",
    "predictions = {}\n",
    "prev = 'company'\n",
    "\n",
    "for token_pos in range(1, len(text_test)):\n",
    "    probabilities = []\n",
    "    for tag in tags:\n",
    "        transition_scores = [\n",
    "            calculate_transition_scores(token_pos, prev, tag, feature_func, normalized_weights)\n",
    "            for i in tags\n",
    "        ]\n",
    "        probabilities.append(softmax(transition_scores)[tags.index(tag)])\n",
    "    \n",
    "#     prev = tags[np.argmax(probabilities)]\n",
    "#     predictions[text_test[token_pos]] = [round(float(prob), 3) for prob in probabilities]  # Convert to float before rounding\n",
    "    \n",
    "    max_prob_index = np.argmax(probabilities)\n",
    "    max_prob_tag = tags[max_prob_index]\n",
    "    max_prob_value = probabilities[max_prob_index]\n",
    "    \n",
    "    prev = max_prob_tag\n",
    "    formatted_max_prob = f\"{max_prob_tag} => {float(max_prob_value):.3f}\"  # Convert to float before formatting\n",
    "    predictions[text_test[token_pos]] = [round(float(prob), 3) for prob in probabilities] + [formatted_max_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2c2cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1c6ac368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Updated weights:  [0.009 0.014 0.019 0.025 0.009 0.011 0.026 0.016 0.026 0.019 0.024 0.014\n",
      " 0.027 0.026 0.02  0.009 0.009 0.019 0.022 0.007 0.011 0.007 0.01  0.009\n",
      " 0.029 0.014 0.013 0.021 0.009 0.013 0.012 0.011 0.017 0.013 0.013 0.011\n",
      " 0.007 0.008 0.022 0.008 0.027 0.012 0.011 0.014 0.027 0.018 0.017 0.02\n",
      " 0.02  0.02  0.007 0.019 0.015 0.013 0.017 0.023 0.008 0.011 0.015 0.017\n",
      " 0.008 0.019 0.008 0.008 0.017]\n",
      "Epoch 1 Updated weights:  [0.005 0.012 0.02  0.031 0.006 0.009 0.033 0.015 0.032 0.02  0.029 0.012\n",
      " 0.035 0.032 0.022 0.006 0.006 0.021 0.025 0.004 0.008 0.005 0.007 0.006\n",
      " 0.038 0.013 0.012 0.023 0.006 0.011 0.01  0.009 0.017 0.011 0.011 0.008\n",
      " 0.004 0.005 0.025 0.005 0.034 0.009 0.009 0.013 0.034 0.019 0.018 0.021\n",
      " 0.022 0.021 0.004 0.02  0.014 0.011 0.017 0.027 0.005 0.009 0.013 0.017\n",
      " 0.005 0.02  0.006 0.006 0.018]\n",
      "Epoch 2 Updated weights:  [0.002 0.009 0.02  0.04  0.003 0.005 0.044 0.012 0.042 0.021 0.035 0.009\n",
      " 0.047 0.041 0.023 0.003 0.003 0.021 0.028 0.002 0.005 0.002 0.004 0.003\n",
      " 0.055 0.01  0.009 0.025 0.003 0.008 0.007 0.005 0.016 0.008 0.008 0.005\n",
      " 0.002 0.002 0.028 0.002 0.046 0.006 0.006 0.01  0.046 0.018 0.016 0.022\n",
      " 0.024 0.022 0.002 0.02  0.011 0.007 0.016 0.031 0.003 0.006 0.011 0.016\n",
      " 0.002 0.019 0.003 0.003 0.016]\n",
      "Epoch 3 Updated weights:  [0.    0.005 0.017 0.051 0.001 0.002 0.06  0.008 0.055 0.018 0.042 0.005\n",
      " 0.066 0.053 0.022 0.001 0.001 0.019 0.029 0.    0.002 0.    0.001 0.001\n",
      " 0.084 0.006 0.005 0.024 0.001 0.004 0.003 0.002 0.012 0.004 0.004 0.002\n",
      " 0.    0.001 0.029 0.001 0.062 0.003 0.002 0.006 0.062 0.015 0.013 0.02\n",
      " 0.023 0.021 0.    0.018 0.007 0.004 0.012 0.035 0.001 0.002 0.007 0.012\n",
      " 0.001 0.017 0.001 0.001 0.013]\n",
      "Epoch 4 Updated weights:  [0.    0.002 0.011 0.06  0.    0.    0.078 0.004 0.068 0.012 0.044 0.002\n",
      " 0.091 0.064 0.017 0.    0.    0.013 0.026 0.    0.    0.    0.    0.\n",
      " 0.132 0.002 0.002 0.019 0.    0.001 0.001 0.    0.007 0.001 0.001 0.\n",
      " 0.    0.    0.026 0.    0.083 0.001 0.001 0.002 0.083 0.009 0.007 0.014\n",
      " 0.018 0.015 0.    0.012 0.003 0.001 0.006 0.034 0.    0.001 0.003 0.006\n",
      " 0.    0.011 0.    0.    0.007]\n",
      "Token      person    date     in    duration    amount     cc    vbd    company\n",
      "hired       0.001   0.001  0.001       0.001     0.001  0.001  0.994      0.001\n",
      "ABC         0.994   0.001  0.001       0.001     0.001  0.001  0.001      0.001\n",
      "in          0.001   0.001  0.001       0.001     0.001  0.994  0.001      0.001\n",
      "1995        0.001   0.994  0.001       0.001     0.001  0.001  0.001      0.001\n",
      "for         0.001   0.001  0.994       0.001     0.001  0.001  0.001      0.001\n",
      "10          0.001   0.001  0.001       0.001     0.994  0.001  0.001      0.001\n",
      "per         0.001   0.001  0.994       0.001     0.001  0.001  0.001      0.001\n",
      "hour        0.001   0.001  0.001       0.994     0.001  0.001  0.001      0.001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import string\n",
    "from tabulate import tabulate\n",
    "\n",
    "LABEL_TYPES = {'company', 'vbd', 'person', 'cc', 'date', 'in', 'amount', 'duration'}\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = ''.join([word for word in text if word not in string.punctuation])\n",
    "    tokens = cleaned_text.split(\" \")\n",
    "    return tokens\n",
    "\n",
    "text_train = \"Google hired XYZ in 2005, for $100,000 per annum.\"\n",
    "text_test = \"Apple hired ABC in 1995, for $10 per hour.\"\n",
    "\n",
    "text_train = clean_text(text_train)\n",
    "text_test = clean_text(text_test)\n",
    "\n",
    "text_train_tags = {\n",
    "    'Google': 'company',\n",
    "    'hired': 'vbd',\n",
    "    'XYZ': 'person',\n",
    "    'in': 'cc',\n",
    "    '2005': 'date',\n",
    "    'for': 'in',\n",
    "    '100000': 'amount',\n",
    "    'per': 'in',\n",
    "    'annum': 'duration'\n",
    "}\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return np.round(e_x / e_x.sum(axis=0), 3)\n",
    "\n",
    "def generate_feature_functions(label_types):\n",
    "    feature_functions = [\n",
    "        lambda position, label1, label2: 1 if position == 0 else 0,\n",
    "    ]\n",
    "\n",
    "    for label1 in label_types:\n",
    "        for label2 in label_types:\n",
    "            feature_functions.append(\n",
    "                lambda position, l1=label1, l2=label2: int(text_train_tags.get(text_train[max(0, position - 1)], '') == l1 and text_train_tags.get(text_train[position], '') == l2)\n",
    "            )\n",
    "\n",
    "    return feature_functions\n",
    "\n",
    "feature_func = generate_feature_functions(LABEL_TYPES)\n",
    "\n",
    "weights = np.random.rand(len(feature_func))\n",
    "np.set_printoptions(suppress=True)\n",
    "epochs = 5\n",
    "learning_rate = 0.05\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for token_pos in range(len(text_train)):\n",
    "        if text_train[token_pos] in text_train_tags:\n",
    "            features = [f(token_pos, text_train_tags.get(text_train[max(0, token_pos - 1)], ''), text_train_tags.get(text_train[token_pos], '')) for f in feature_func]\n",
    "            weights += learning_rate * np.multiply(weights, features)\n",
    "\n",
    "    print(f\"Epoch {epoch} Updated weights: \", softmax(weights))\n",
    "\n",
    "normalized_weights = preprocessing.normalize([weights])\n",
    "\n",
    "tag_labels = list(LABEL_TYPES)\n",
    "predictions = {}\n",
    "previous_tag = 'company'\n",
    "\n",
    "for token_pos in range(1, len(text_test)):\n",
    "    probabilities = []\n",
    "    for tag in tag_labels:\n",
    "        features = [f(token_pos, previous_tag, tag) for f in feature_func]\n",
    "        weighted_features = np.multiply(normalized_weights, features)\n",
    "        probability = sum(weighted_features[0])\n",
    "        probabilities.append(probability)\n",
    "\n",
    "    probabilities = softmax(probabilities)\n",
    "    previous_tag = tag_labels[np.argmax(probabilities)]\n",
    "    predictions[text_test[token_pos]] = [round(prob, 3) for prob in probabilities]\n",
    "\n",
    "table_data = []\n",
    "for token, probs in predictions.items():\n",
    "    table_data.append([token] + probs)\n",
    "\n",
    "headers = [\"Token\"] + tag_labels\n",
    "table = tabulate(table_data, headers=headers, tablefmt=\"plain\")\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7641c32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e3955fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data : ['Google', 'hired', 'XYZ', 'in', '2005', 'for', '100000', 'per', 'annum']\n",
      "Test data : ['Apple', 'hired', 'ABC', 'in', '1995', 'for', '10', 'per', 'hour']\n",
      "Epoch 0 Updated weights:  [0.015 0.012 0.026 0.017 0.018 0.007 0.008 0.015 0.02  0.011 0.012 0.018\n",
      " 0.007 0.022 0.017 0.016 0.02  0.008 0.027 0.009 0.018 0.016 0.021 0.018\n",
      " 0.016 0.015 0.008 0.027 0.018 0.015 0.011 0.008 0.01  0.014 0.016 0.027\n",
      " 0.011 0.018 0.02  0.008 0.009 0.014 0.008 0.022 0.015 0.022 0.012 0.018\n",
      " 0.014 0.018 0.016 0.016 0.012 0.016 0.026 0.011 0.015 0.007 0.008 0.013\n",
      " 0.009 0.022 0.025 0.01  0.022]\n",
      "Epoch 1 Updated weights:  [0.009 0.01  0.032 0.017 0.018 0.004 0.005 0.014 0.021 0.009 0.01  0.018\n",
      " 0.004 0.025 0.017 0.015 0.023 0.006 0.036 0.007 0.018 0.016 0.024 0.019\n",
      " 0.016 0.015 0.005 0.035 0.019 0.014 0.009 0.006 0.007 0.013 0.016 0.035\n",
      " 0.009 0.019 0.022 0.005 0.006 0.012 0.006 0.025 0.014 0.026 0.011 0.019\n",
      " 0.013 0.019 0.015 0.015 0.01  0.015 0.032 0.008 0.014 0.005 0.006 0.011\n",
      " 0.007 0.026 0.031 0.007 0.025]\n",
      "Epoch 2 Updated weights:  [0.004 0.007 0.043 0.016 0.018 0.002 0.003 0.012 0.023 0.006 0.007 0.018\n",
      " 0.002 0.029 0.017 0.013 0.025 0.003 0.051 0.004 0.018 0.015 0.027 0.019\n",
      " 0.014 0.013 0.002 0.048 0.018 0.012 0.006 0.003 0.004 0.011 0.015 0.048\n",
      " 0.006 0.02  0.024 0.002 0.003 0.01  0.003 0.03  0.012 0.031 0.008 0.019\n",
      " 0.011 0.019 0.013 0.014 0.007 0.014 0.043 0.005 0.012 0.002 0.003 0.008\n",
      " 0.004 0.031 0.04  0.004 0.029]\n",
      "Epoch 3 Updated weights:  [0.001 0.003 0.06  0.013 0.015 0.    0.001 0.008 0.022 0.003 0.004 0.015\n",
      " 0.    0.032 0.014 0.01  0.026 0.001 0.078 0.001 0.015 0.011 0.03  0.016\n",
      " 0.01  0.009 0.001 0.071 0.016 0.008 0.003 0.001 0.002 0.007 0.011 0.072\n",
      " 0.003 0.018 0.024 0.001 0.001 0.006 0.001 0.033 0.009 0.036 0.004 0.017\n",
      " 0.007 0.017 0.01  0.01  0.004 0.01  0.06  0.002 0.008 0.001 0.001 0.004\n",
      " 0.001 0.036 0.053 0.002 0.032]\n",
      "Epoch 4 Updated weights:  [0.    0.001 0.082 0.007 0.009 0.    0.    0.003 0.018 0.001 0.001 0.009\n",
      " 0.    0.031 0.008 0.005 0.023 0.    0.123 0.    0.01  0.006 0.028 0.011\n",
      " 0.005 0.004 0.    0.107 0.011 0.004 0.001 0.    0.    0.003 0.006 0.108\n",
      " 0.001 0.012 0.019 0.    0.    0.002 0.    0.033 0.004 0.036 0.001 0.012\n",
      " 0.003 0.012 0.005 0.005 0.001 0.005 0.081 0.001 0.004 0.    0.    0.001\n",
      " 0.    0.037 0.067 0.    0.032]\n",
      "Token      person    date     in    duration    amount     cc    vbd    company\n",
      "hired       0.001   0.001  0.001       0.001     0.001  0.001  0.995      0.001\n",
      "ABC         0.995   0.001  0.001       0.001     0.001  0.001  0.001      0.001\n",
      "in          0.001   0.001  0.001       0.001     0.001  0.995  0.001      0.001\n",
      "1995        0.001   0.995  0.001       0.001     0.001  0.001  0.001      0.001\n",
      "for         0.001   0.001  0.995       0.001     0.001  0.001  0.001      0.001\n",
      "10          0.001   0.001  0.001       0.001     0.995  0.001  0.001      0.001\n",
      "per         0.001   0.001  0.995       0.001     0.001  0.001  0.001      0.001\n",
      "hour        0.001   0.001  0.001       0.995     0.001  0.001  0.001      0.001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import string\n",
    "from tabulate import tabulate\n",
    "\n",
    "LABEL_TYPES = {'company', 'vbd', 'person', 'cc', 'date', 'in', 'amount', 'duration'}\n",
    "\n",
    "text_train = \"Google hired XYZ in 2005, for $100,000 per annum.\"\n",
    "text_train = ''.join([word for word in text_train if word not in string.punctuation]).split(\" \")\n",
    "\n",
    "text_train_tags = {\n",
    "    'Google': 'company',\n",
    "    'hired': 'vbd',\n",
    "    'XYZ': 'person',\n",
    "    'in': 'cc',\n",
    "    '2005': 'date',\n",
    "    'for': 'in',\n",
    "    '100000': 'amount',\n",
    "    'per': 'in',\n",
    "    'annum': 'duration'\n",
    "}\n",
    "\n",
    "text_test = 'Apple hired ABC in 1995, for $10 per hour.'\n",
    "text_test = ''.join([word for word in text_test if word not in string.punctuation]).split(\" \")\n",
    "\n",
    "print(f\"Train data : {text_train}\\nTest data : {text_test}\")\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return np.round(e_x / e_x.sum(axis=0), 3)\n",
    "\n",
    "def generate_feature_functions(label_types):\n",
    "    feature_functions = [\n",
    "        lambda position, label1, label2: 1 if position == 0 else 0,\n",
    "    ]\n",
    "\n",
    "    for label1 in label_types:\n",
    "        for label2 in label_types:\n",
    "            feature_functions.append(\n",
    "                lambda position, l1=label1, l2=label2: 1 if text_train_tags.get(text_train[max(0, position - 1)], '') == l1 and text_train_tags.get(text_train[position], '') == l2 else 0\n",
    "            )\n",
    "\n",
    "    return feature_functions\n",
    "\n",
    "feature_func = generate_feature_functions(LABEL_TYPES)\n",
    "\n",
    "weights = np.random.rand(len(feature_func))\n",
    "np.set_printoptions(suppress=True)\n",
    "epochs = 5\n",
    "learning_rate = 0.05\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for token_pos in range(len(text_train)):\n",
    "        if text_train[token_pos] in text_train_tags:\n",
    "            features = [f(token_pos, text_train_tags.get(text_train[max(0, token_pos - 1)], ''), text_train_tags.get(text_train[token_pos], '')) for f in feature_func]\n",
    "            weights += learning_rate * np.multiply(weights, features)\n",
    "\n",
    "    print(f\"Epoch {epoch} Updated weights: \", softmax(weights))\n",
    "\n",
    "weights = preprocessing.normalize([weights])\n",
    "\n",
    "tags = list(LABEL_TYPES)\n",
    "pred_dict = {}\n",
    "prev = 'company'\n",
    "\n",
    "for token_pos in range(1, len(text_test)):\n",
    "    probabilities = []\n",
    "    for tag in tags:\n",
    "        features = [f(token_pos, prev, tag) for f in feature_func]\n",
    "        weighted_features = np.multiply(weights, features)\n",
    "        probability = sum(weighted_features[0])\n",
    "        probabilities.append(probability)\n",
    "\n",
    "    probabilities = softmax(probabilities)\n",
    "    prev = tags[np.argmax(probabilities)]\n",
    "    pred_dict[text_test[token_pos]] = [round(prob, 3) for prob in probabilities]\n",
    "\n",
    "table_data = []\n",
    "for token, probs in pred_dict.items():\n",
    "    table_data.append([token] + probs)\n",
    "\n",
    "headers = [\"Token\"] + tags\n",
    "table = tabulate(table_data, headers=headers, tablefmt=\"plain\")\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4cfc5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62845d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d5a075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d826639e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1616b15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68689714",
   "metadata": {},
   "source": [
    "## Final Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "776cc266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data : ['Google', 'hired', 'XYZ', 'in', '2005', 'for', '100000', 'per', 'annum']\n",
      "Test data : ['Apple', 'hired', 'ABC', 'in', '1995', 'for', '10', 'per', 'hour']\n",
      "Epoch 0 Updated weights:  [0.013 0.022 0.019 0.009 0.021 0.029 0.017 0.028 0.01  0.013 0.008 0.019\n",
      " 0.023 0.019 0.012 0.008 0.018 0.015 0.009 0.012 0.014 0.007 0.012 0.018\n",
      " 0.012 0.013 0.01  0.01  0.016 0.015 0.017 0.009 0.028 0.01  0.013 0.006\n",
      " 0.015 0.008 0.022 0.026 0.013 0.013 0.022 0.019 0.007 0.015 0.021 0.021\n",
      " 0.022 0.026 0.019 0.012 0.017 0.021 0.02  0.009 0.016 0.019 0.024 0.008\n",
      " 0.01  0.009 0.011 0.013 0.007]\n",
      "Epoch 1 Updated weights:  [0.008 0.025 0.02  0.007 0.023 0.038 0.016 0.038 0.008 0.011 0.005 0.02\n",
      " 0.027 0.019 0.01  0.005 0.018 0.014 0.007 0.009 0.012 0.004 0.01  0.019\n",
      " 0.01  0.012 0.007 0.008 0.016 0.013 0.016 0.007 0.038 0.007 0.012 0.004\n",
      " 0.014 0.005 0.025 0.034 0.011 0.011 0.026 0.021 0.004 0.014 0.024 0.023\n",
      " 0.025 0.032 0.021 0.01  0.016 0.024 0.022 0.006 0.016 0.02  0.029 0.005\n",
      " 0.008 0.007 0.009 0.011 0.004]\n",
      "Epoch 2 Updated weights:  [0.003 0.029 0.021 0.004 0.025 0.056 0.015 0.054 0.004 0.008 0.002 0.02\n",
      " 0.032 0.019 0.007 0.003 0.017 0.012 0.004 0.006 0.009 0.002 0.007 0.018\n",
      " 0.007 0.009 0.004 0.005 0.014 0.011 0.015 0.004 0.054 0.004 0.009 0.002\n",
      " 0.012 0.002 0.029 0.045 0.008 0.008 0.031 0.022 0.002 0.011 0.027 0.026\n",
      " 0.028 0.043 0.021 0.007 0.015 0.027 0.023 0.003 0.014 0.021 0.035 0.002\n",
      " 0.005 0.004 0.006 0.008 0.002]\n",
      "Epoch 3 Updated weights:  [0.001 0.032 0.019 0.001 0.025 0.086 0.011 0.082 0.002 0.004 0.001 0.017\n",
      " 0.036 0.017 0.004 0.001 0.014 0.008 0.001 0.003 0.005 0.    0.004 0.015\n",
      " 0.004 0.005 0.001 0.002 0.01  0.007 0.011 0.001 0.082 0.001 0.005 0.\n",
      " 0.008 0.001 0.032 0.062 0.005 0.005 0.034 0.02  0.    0.007 0.029 0.026\n",
      " 0.03  0.057 0.019 0.003 0.011 0.028 0.022 0.001 0.01  0.019 0.043 0.001\n",
      " 0.002 0.001 0.003 0.004 0.   ]\n",
      "Epoch 4 Updated weights:  [0.    0.029 0.013 0.    0.02  0.137 0.005 0.126 0.    0.001 0.    0.011\n",
      " 0.036 0.011 0.001 0.    0.008 0.003 0.    0.001 0.002 0.    0.001 0.009\n",
      " 0.001 0.002 0.    0.    0.005 0.003 0.006 0.    0.126 0.    0.002 0.\n",
      " 0.004 0.    0.029 0.083 0.001 0.001 0.032 0.014 0.    0.003 0.025 0.022\n",
      " 0.027 0.073 0.014 0.001 0.005 0.024 0.017 0.    0.005 0.013 0.046 0.\n",
      " 0.    0.    0.001 0.001 0.   ]\n",
      "Token: hired, Max Probability Label: vbd, Max Probability: 0.995\n",
      "Token: ABC, Max Probability Label: person, Max Probability: 0.995\n",
      "Token: in, Max Probability Label: cc, Max Probability: 0.995\n",
      "Token: 1995, Max Probability Label: date, Max Probability: 0.995\n",
      "Token: for, Max Probability Label: in, Max Probability: 0.995\n",
      "Token: 10, Max Probability Label: amount, Max Probability: 0.995\n",
      "Token: per, Max Probability Label: in, Max Probability: 0.995\n",
      "Token: hour, Max Probability Label: duration, Max Probability: 0.995\n",
      "Token      person    date     in    duration    amount     cc    vbd    company  Max Prob Tag\n",
      "hired       0.001   0.001  0.001       0.001     0.001  0.001  0.995      0.001  vbd => 0.995\n",
      "ABC         0.995   0.001  0.001       0.001     0.001  0.001  0.001      0.001  person => 0.995\n",
      "in          0.001   0.001  0.001       0.001     0.001  0.995  0.001      0.001  cc => 0.995\n",
      "1995        0.001   0.995  0.001       0.001     0.001  0.001  0.001      0.001  date => 0.995\n",
      "for         0.001   0.001  0.995       0.001     0.001  0.001  0.001      0.001  in => 0.995\n",
      "10          0.001   0.001  0.001       0.001     0.995  0.001  0.001      0.001  amount => 0.995\n",
      "per         0.001   0.001  0.995       0.001     0.001  0.001  0.001      0.001  in => 0.995\n",
      "hour        0.001   0.001  0.001       0.995     0.001  0.001  0.001      0.001  duration => 0.995\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import string\n",
    "from tabulate import tabulate\n",
    "\n",
    "LABEL_TYPES = {'company', 'vbd', 'person', 'cc', 'date', 'in', 'amount', 'duration'}\n",
    "\n",
    "text_train = \"Google hired XYZ in 2005, for $100,000 per annum.\"\n",
    "text_train = ''.join([word for word in text_train if word not in string.punctuation]).split(\" \")\n",
    "\n",
    "text_train_tags = {\n",
    "    'Google': 'company',\n",
    "    'hired': 'vbd',\n",
    "    'XYZ': 'person',\n",
    "    'in': 'cc',\n",
    "    '2005': 'date',\n",
    "    'for': 'in',\n",
    "    '100000': 'amount',\n",
    "    'per': 'in',\n",
    "    'annum': 'duration'\n",
    "}\n",
    "\n",
    "text_test = 'Apple hired ABC in 1995, for $10 per hour.'\n",
    "text_test = ''.join([word for word in text_test if word not in string.punctuation]).split(\" \")\n",
    "\n",
    "print(f\"Train data : {text_train}\\nTest data : {text_test}\")\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return np.round(e_x / e_x.sum(axis=0), 3)\n",
    "\n",
    "def generate_feature_functions(label_types):\n",
    "    feature_functions = [\n",
    "        lambda position, label1, label2: 1 if position == 0 else 0,\n",
    "    ]\n",
    "\n",
    "    for label1 in label_types:\n",
    "        for label2 in label_types:\n",
    "            feature_functions.append(\n",
    "                lambda position, l1=label1, l2=label2: 1 if text_train_tags.get(text_train[max(0, position - 1)], '') == l1 and text_train_tags.get(text_train[position], '') == l2 else 0\n",
    "            )\n",
    "\n",
    "    return feature_functions\n",
    "\n",
    "feature_func = generate_feature_functions(LABEL_TYPES)\n",
    "\n",
    "weights = np.random.rand(len(feature_func))\n",
    "np.set_printoptions(suppress=True)\n",
    "epochs = 5\n",
    "learning_rate = 0.05\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for token_pos in range(len(text_train)):\n",
    "        if text_train[token_pos] in text_train_tags:\n",
    "            features = [f(token_pos, text_train_tags.get(text_train[max(0, token_pos - 1)], ''), text_train_tags.get(text_train[token_pos], '')) for f in feature_func]\n",
    "            weights += learning_rate * np.multiply(weights, features)\n",
    "\n",
    "    print(f\"Epoch {epoch} Updated weights: \", softmax(weights))\n",
    "\n",
    "weights = preprocessing.normalize([weights])\n",
    "\n",
    "tags = list(LABEL_TYPES)\n",
    "pred_dict = {}\n",
    "prev = 'company'\n",
    "\n",
    "for token_pos in range(1, len(text_test)):\n",
    "    probabilities = []\n",
    "    for tag in tags:\n",
    "        features = [f(token_pos, prev, tag) for f in feature_func]\n",
    "        weighted_features = np.multiply(weights, features)\n",
    "        probability = sum(weighted_features[0])\n",
    "        probabilities.append(probability)\n",
    "\n",
    "    probabilities = softmax(probabilities)\n",
    "    prev = tags[np.argmax(probabilities)]\n",
    "    max_prob_label = tags[np.argmax(probabilities)]\n",
    "    max_prob_value = max(probabilities)\n",
    "    max_prob_tag = f\"{max_prob_label} => {max_prob_value}\"\n",
    "    pred_dict[text_test[token_pos]] = [round(prob, 3) for prob in probabilities] + [max_prob_tag]\n",
    "    print(f\"Token: {text_test[token_pos]}, Max Probability Label: {max_prob_label}, Max Probability: {max(probabilities)}\")\n",
    "\n",
    "table_data = []\n",
    "for token, probs in pred_dict.items():\n",
    "    table_data.append([token] + probs)\n",
    "\n",
    "headers = [\"Token\"] + tags + [\"Max Prob Tag\"]\n",
    "table = tabulate(table_data, headers=headers, tablefmt=\"plain\")\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aa0fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
