{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59990e4e",
   "metadata": {},
   "source": [
    "## PCA code from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "623ce7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb6ab1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Load Breast Cancer Dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e708331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (569, 30)\n",
      "First few rows of the dataset:\n",
      "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
      "0        17.99         10.38          122.80     1001.0          0.11840   \n",
      "1        20.57         17.77          132.90     1326.0          0.08474   \n",
      "2        19.69         21.25          130.00     1203.0          0.10960   \n",
      "3        11.42         20.38           77.58      386.1          0.14250   \n",
      "4        20.29         14.34          135.10     1297.0          0.10030   \n",
      "\n",
      "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
      "0           0.27760          0.3001              0.14710         0.2419   \n",
      "1           0.07864          0.0869              0.07017         0.1812   \n",
      "2           0.15990          0.1974              0.12790         0.2069   \n",
      "3           0.28390          0.2414              0.10520         0.2597   \n",
      "4           0.13280          0.1980              0.10430         0.1809   \n",
      "\n",
      "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
      "0                 0.07871  ...         25.38          17.33           184.60   \n",
      "1                 0.05667  ...         24.99          23.41           158.80   \n",
      "2                 0.05999  ...         23.57          25.53           152.50   \n",
      "3                 0.09744  ...         14.91          26.50            98.87   \n",
      "4                 0.05883  ...         22.54          16.67           152.20   \n",
      "\n",
      "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
      "0      2019.0            0.1622             0.6656           0.7119   \n",
      "1      1956.0            0.1238             0.1866           0.2416   \n",
      "2      1709.0            0.1444             0.4245           0.4504   \n",
      "3       567.7            0.2098             0.8663           0.6869   \n",
      "4      1575.0            0.1374             0.2050           0.4000   \n",
      "\n",
      "   worst concave points  worst symmetry  worst fractal dimension  \n",
      "0                0.2654          0.4601                  0.11890  \n",
      "1                0.1860          0.2750                  0.08902  \n",
      "2                0.2430          0.3613                  0.08758  \n",
      "3                0.2575          0.6638                  0.17300  \n",
      "4                0.1625          0.2364                  0.07678  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# Task 2: Print shape and first few rows\n",
    "print(\"Dataset shape:\", X.shape)\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(pd.DataFrame(X, columns=data.feature_names).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2605bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Define features and target\n",
    "features = X\n",
    "target = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c2312b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names:\n",
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n"
     ]
    }
   ],
   "source": [
    "# Task 4: Print the feature names\n",
    "print(\"Feature names:\")\n",
    "print(data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "559eb72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: Standardize the input\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af4c0ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of standardized features: -6.118909323768877e-16\n",
      "Standard deviation of standardized features: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Task 6: Print the mean and std of the transformed input\n",
    "print(\"Mean of standardized features:\", np.mean(features))\n",
    "print(\"Standard deviation of standardized features:\", np.std(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "834ec959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 7: Define the number of principal components to 2\n",
    "n_components = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f22fca9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16820\\2314687733.py:6: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  w = np.hstack((eigen_pairs[i][1].reshape(len(eigenvalues), 1)) for i in range(n_components))\n"
     ]
    }
   ],
   "source": [
    "# Task 8: Fit the input into PCA\n",
    "covariance_matrix = np.cov(features.T)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n",
    "eigen_pairs = [(np.abs(eigenvalues[i]), eigenvectors[:, i]) for i in range(len(eigenvalues))]\n",
    "eigen_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "w = np.hstack((eigen_pairs[i][1].reshape(len(eigenvalues), 1)) for i in range(n_components))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fca70bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance ratio of the components:\n",
      "[4.42720256e-01 1.89711820e-01 9.39316326e-02 6.60213492e-02\n",
      " 5.49576849e-02 4.02452204e-02 2.25073371e-02 1.58872380e-02\n",
      " 1.38964937e-02 1.16897819e-02 9.79718988e-03 8.70537901e-03\n",
      " 8.04524987e-03 5.23365745e-03 3.13783217e-03 2.66209337e-03\n",
      " 1.97996793e-03 1.75395945e-03 1.64925306e-03 4.43482743e-06\n",
      " 2.49601032e-05 5.29779290e-05 2.30015463e-04 2.72587995e-04\n",
      " 5.16042379e-04 6.01833567e-04 8.11361259e-04 9.14646751e-04\n",
      " 1.03864675e-03 9.99096464e-04]\n"
     ]
    }
   ],
   "source": [
    "# Task 9: Print the variance ratio of the components\n",
    "variance_ratio = eigenvalues / sum(eigenvalues)\n",
    "print(\"Variance ratio of the components:\")\n",
    "print(variance_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4123a5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 10: Divide the dataset into training set and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "979e6397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 11: Build neural networks for dataset without PCA and with PCA\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.num_layers = len(hidden_dims) + 1\n",
    "\n",
    "        # Initialize weights and biases for hidden layers\n",
    "        for i in range(self.num_layers - 1):\n",
    "            if i == 0:\n",
    "                self.weights.append(np.random.randn(input_dim, hidden_dims[i]))\n",
    "            else:\n",
    "                self.weights.append(np.random.randn(hidden_dims[i-1], hidden_dims[i]))\n",
    "            self.biases.append(np.zeros(hidden_dims[i]))\n",
    "\n",
    "        # Initialize weights and biases for the output layer\n",
    "        self.weights.append(np.random.randn(hidden_dims[-1], output_dim))\n",
    "        self.biases.append(np.zeros(output_dim))\n",
    "\n",
    "    def forward(self, X):\n",
    "        activations = []\n",
    "        for i in range(self.num_layers):\n",
    "            if i == 0:\n",
    "                activations.append(self.relu(np.dot(X, self.weights[i]) + self.biases[i]))\n",
    "            else:\n",
    "                activations.append(self.relu(np.dot(activations[i-1], self.weights[i]) + self.biases[i]))\n",
    "        return activations[-1]\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def predict(self, X):\n",
    "        output = self.forward(X)\n",
    "        return np.round(output)\n",
    "\n",
    "    def get_accuracy(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        accuracy = np.mean(predictions == y)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bb56c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 12: Total layers – 1 input layer, 2 hidden layers of 500 neurons with ReLU activation, 1 output layer with 1 neuron and sigmoid activation\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dims = [500, 500]\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49e22f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without PCA\n",
    "nn_without_pca = NeuralNetwork(input_dim, hidden_dims, output_dim)\n",
    "\n",
    "# With PCA\n",
    "nn_with_pca = NeuralNetwork(n_components, hidden_dims, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2988533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 13: Compile the model with loss as Cross entropy, adam optimizer, and accuracy metric\n",
    "class BinaryCrossEntropyLoss:\n",
    "    @staticmethod\n",
    "    def loss(y_true, y_pred):\n",
    "        epsilon = 1e-10\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        loss = -y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred)\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient(y_true, y_pred):\n",
    "        epsilon = 1e-10\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        gradient = (y_pred - y_true) / (y_pred * (1 - y_pred))\n",
    "        return gradient\n",
    "\n",
    "\n",
    "class AdamOptimizer:\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.t = 0\n",
    "\n",
    "    def update(self, weights, biases, gradients):\n",
    "        if self.m is None and self.v is None:\n",
    "            self.m = [np.zeros_like(weight) for weight in weights]\n",
    "            self.v = [np.zeros_like(weight) for weight in weights]\n",
    "\n",
    "        self.t += 1\n",
    "        for i in range(len(weights)):\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * gradients[i]\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * np.square(gradients[i])\n",
    "\n",
    "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            weights[i] -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "            biases[i] -= self.learning_rate * np.mean(m_hat / (np.sqrt(v_hat) + self.epsilon))\n",
    "\n",
    "\n",
    "def train_network(nn, X_train, y_train, num_epochs=100, batch_size=32, learning_rate=0.001):\n",
    "    loss_func = BinaryCrossEntropyLoss()\n",
    "    optimizer = AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "    num_samples = X_train.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in range(num_batches):\n",
    "            start = batch * batch_size\n",
    "            end = start + batch_size\n",
    "            X_batch = X_train[start:end]\n",
    "            y_batch = y_train[start:end]\n",
    "\n",
    "            activations = []\n",
    "            activations.append(X_batch)\n",
    "            output = nn.forward(X_batch)\n",
    "            activations.append(output)\n",
    "\n",
    "            loss = loss_func.loss(y_batch, output)\n",
    "            gradient = loss_func.gradient(y_batch, output)\n",
    "\n",
    "            for i in range(nn.num_layers - 1, -1, -1):\n",
    "                if i == nn.num_layers - 1:\n",
    "                    delta = gradient\n",
    "                else:\n",
    "                    delta = np.dot(delta, nn.weights[i + 1].T) * (activations[i] > 0)\n",
    "\n",
    "                weight_gradient = np.dot(activations[i].T, delta)\n",
    "                bias_gradient = np.mean(delta, axis=0)\n",
    "\n",
    "                optimizer.update(nn.weights, nn.biases, [weight_gradient, bias_gradient])\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "690c4860",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m      4\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[1;32m----> 6\u001b[0m nn_without_pca \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_without_pca\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m nn_with_pca \u001b[38;5;241m=\u001b[39m train_network(nn_with_pca, X_train \u001b[38;5;241m@\u001b[39m w, y_train, num_epochs, batch_size, learning_rate)\n\u001b[0;32m      9\u001b[0m accuracy_without_pca \u001b[38;5;241m=\u001b[39m nn_without_pca\u001b[38;5;241m.\u001b[39mget_accuracy(X_test, y_test)\n",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36mtrain_network\u001b[1;34m(nn, X_train, y_train, num_epochs, batch_size, learning_rate)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     delta \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(delta, nn\u001b[38;5;241m.\u001b[39mweights[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m*\u001b[39m (activations[i] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 73\u001b[0m weight_gradient \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(\u001b[43mactivations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mT, delta)\n\u001b[0;32m     74\u001b[0m bias_gradient \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(delta, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     76\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mupdate(nn\u001b[38;5;241m.\u001b[39mweights, nn\u001b[38;5;241m.\u001b[39mbiases, [weight_gradient, bias_gradient])\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Task 14: Print the accuracy for both networks\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "nn_without_pca = train_network(nn_without_pca, X_train, y_train, num_epochs, batch_size, learning_rate)\n",
    "nn_with_pca = train_network(nn_with_pca, X_train @ w, y_train, num_epochs, batch_size, learning_rate)\n",
    "\n",
    "accuracy_without_pca = nn_without_pca.get_accuracy(X_test, y_test)\n",
    "accuracy_with_pca = nn_with_pca.get_accuracy(X_test @ w, y_test)\n",
    "\n",
    "print(\"Accuracy without PCA:\", accuracy_without_pca)\n",
    "print(\"Accuracy with PCA:\", accuracy_with_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0940c844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aef3447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a1a1a72",
   "metadata": {},
   "source": [
    "## using PCA library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb13423d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82566e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (569, 30)\n",
      "First few rows of the dataset:\n",
      "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
      "0        17.99         10.38          122.80     1001.0          0.11840   \n",
      "1        20.57         17.77          132.90     1326.0          0.08474   \n",
      "2        19.69         21.25          130.00     1203.0          0.10960   \n",
      "3        11.42         20.38           77.58      386.1          0.14250   \n",
      "4        20.29         14.34          135.10     1297.0          0.10030   \n",
      "\n",
      "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
      "0           0.27760          0.3001              0.14710         0.2419   \n",
      "1           0.07864          0.0869              0.07017         0.1812   \n",
      "2           0.15990          0.1974              0.12790         0.2069   \n",
      "3           0.28390          0.2414              0.10520         0.2597   \n",
      "4           0.13280          0.1980              0.10430         0.1809   \n",
      "\n",
      "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
      "0                 0.07871  ...         25.38          17.33           184.60   \n",
      "1                 0.05667  ...         24.99          23.41           158.80   \n",
      "2                 0.05999  ...         23.57          25.53           152.50   \n",
      "3                 0.09744  ...         14.91          26.50            98.87   \n",
      "4                 0.05883  ...         22.54          16.67           152.20   \n",
      "\n",
      "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
      "0      2019.0            0.1622             0.6656           0.7119   \n",
      "1      1956.0            0.1238             0.1866           0.2416   \n",
      "2      1709.0            0.1444             0.4245           0.4504   \n",
      "3       567.7            0.2098             0.8663           0.6869   \n",
      "4      1575.0            0.1374             0.2050           0.4000   \n",
      "\n",
      "   worst concave points  worst symmetry  worst fractal dimension  \n",
      "0                0.2654          0.4601                  0.11890  \n",
      "1                0.1860          0.2750                  0.08902  \n",
      "2                0.2430          0.3613                  0.08758  \n",
      "3                0.2575          0.6638                  0.17300  \n",
      "4                0.1625          0.2364                  0.07678  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "Feature names:\n",
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "Mean of standardized features: -6.118909323768877e-16\n",
      "Standard deviation of standardized features: 1.0\n",
      "Variance ratio of the components:\n",
      "[0.44272026 0.18971182]\n",
      "Accuracy without PCA: N/A\n",
      "Accuracy with PCA: N/A\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# Task 1: Load Breast Cancer Dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "\n",
    "# Task 2: Print shape and first few rows\n",
    "print(\"Dataset shape:\", X.shape)\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(pd.DataFrame(X, columns=data.feature_names).head())\n",
    "\n",
    "\n",
    "# Task 3: Define features and target\n",
    "features = X\n",
    "target = y\n",
    "\n",
    "\n",
    "# Task 4: Print the feature names\n",
    "print(\"Feature names:\")\n",
    "print(data.feature_names)\n",
    "\n",
    "\n",
    "# Task 5: Standardize the input\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "\n",
    "# Task 6: Print the mean and std of the transformed input\n",
    "print(\"Mean of standardized features:\", np.mean(features))\n",
    "print(\"Standard deviation of standardized features:\", np.std(features))\n",
    "\n",
    "\n",
    "# Task 7: Define the number of principal components to 2\n",
    "n_components = 2\n",
    "\n",
    "\n",
    "# Task 8: Fit the input into PCA\n",
    "pca = PCA(n_components=n_components)\n",
    "principal_components = pca.fit_transform(features)\n",
    "\n",
    "\n",
    "# Task 9: Print the variance ratio of the components\n",
    "variance_ratio = pca.explained_variance_ratio_\n",
    "print(\"Variance ratio of the components:\")\n",
    "print(variance_ratio)\n",
    "\n",
    "\n",
    "# Task 10: Divide the dataset into training set and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "# Task 11: Build neural networks for dataset without PCA and with PCA\n",
    "# No neural network code in this version\n",
    "\n",
    "\n",
    "# Task 12: Print the accuracy for both networks\n",
    "print(\"Accuracy without PCA: N/A\")\n",
    "print(\"Accuracy with PCA: N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a43b0bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db127ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (569, 30)\n",
      "First few rows of data:\n",
      "    mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
      "0        17.99         10.38          122.80     1001.0          0.11840   \n",
      "1        20.57         17.77          132.90     1326.0          0.08474   \n",
      "2        19.69         21.25          130.00     1203.0          0.10960   \n",
      "3        11.42         20.38           77.58      386.1          0.14250   \n",
      "4        20.29         14.34          135.10     1297.0          0.10030   \n",
      "\n",
      "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
      "0           0.27760          0.3001              0.14710         0.2419   \n",
      "1           0.07864          0.0869              0.07017         0.1812   \n",
      "2           0.15990          0.1974              0.12790         0.2069   \n",
      "3           0.28390          0.2414              0.10520         0.2597   \n",
      "4           0.13280          0.1980              0.10430         0.1809   \n",
      "\n",
      "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
      "0                 0.07871  ...         25.38          17.33           184.60   \n",
      "1                 0.05667  ...         24.99          23.41           158.80   \n",
      "2                 0.05999  ...         23.57          25.53           152.50   \n",
      "3                 0.09744  ...         14.91          26.50            98.87   \n",
      "4                 0.05883  ...         22.54          16.67           152.20   \n",
      "\n",
      "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
      "0      2019.0            0.1622             0.6656           0.7119   \n",
      "1      1956.0            0.1238             0.1866           0.2416   \n",
      "2      1709.0            0.1444             0.4245           0.4504   \n",
      "3       567.7            0.2098             0.8663           0.6869   \n",
      "4      1575.0            0.1374             0.2050           0.4000   \n",
      "\n",
      "   worst concave points  worst symmetry  worst fractal dimension  \n",
      "0                0.2654          0.4601                  0.11890  \n",
      "1                0.1860          0.2750                  0.08902  \n",
      "2                0.2430          0.3613                  0.08758  \n",
      "3                0.2575          0.6638                  0.17300  \n",
      "4                0.1625          0.2364                  0.07678  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "Feature names:\n",
      "- mean radius\n",
      "- mean texture\n",
      "- mean perimeter\n",
      "- mean area\n",
      "- mean smoothness\n",
      "- mean compactness\n",
      "- mean concavity\n",
      "- mean concave points\n",
      "- mean symmetry\n",
      "- mean fractal dimension\n",
      "- radius error\n",
      "- texture error\n",
      "- perimeter error\n",
      "- area error\n",
      "- smoothness error\n",
      "- compactness error\n",
      "- concavity error\n",
      "- concave points error\n",
      "- symmetry error\n",
      "- fractal dimension error\n",
      "- worst radius\n",
      "- worst texture\n",
      "- worst perimeter\n",
      "- worst area\n",
      "- worst smoothness\n",
      "- worst compactness\n",
      "- worst concavity\n",
      "- worst concave points\n",
      "- worst symmetry\n",
      "- worst fractal dimension\n",
      "Mean of transformed input: -6.118909323768877e-16\n",
      "Standard deviation of transformed input: 1.0\n",
      "Variance ratio of the components: [0.44272026 0.18971182]\n",
      "Accuracy without PCA: 0.9680851063829787\n",
      "Accuracy with PCA: 0.9574468085106383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Print shape and first few rows\n",
    "print(\"Data shape:\", X.shape)\n",
    "print(\"First few rows of data:\\n\", pd.DataFrame(X, columns=data.feature_names).head())\n",
    "\n",
    "# Define features and target\n",
    "features = data.feature_names\n",
    "target = data.target_names\n",
    "\n",
    "# Print feature names\n",
    "print(\"Feature names:\")\n",
    "for feature in features:\n",
    "    print(\"-\", feature)\n",
    "\n",
    "# Standardize the input features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Print mean and std of transformed input\n",
    "print(\"Mean of transformed input:\", np.mean(X_scaled))\n",
    "print(\"Standard deviation of transformed input:\", np.std(X_scaled))\n",
    "\n",
    "# Define the number of principal components to 2\n",
    "n_components = 2\n",
    "\n",
    "# Fit the input to PCA\n",
    "pca = PCA(n_components=n_components)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Print variance ratio of the components\n",
    "print(\"Variance ratio of the components:\", pca.explained_variance_ratio_)\n",
    "\n",
    "# Divide the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Build neural network without PCA\n",
    "model_without_pca = MLPClassifier(hidden_layer_sizes=(500, 500), activation='relu', random_state=42)\n",
    "model_without_pca.fit(X_train, y_train)\n",
    "y_pred_without_pca = model_without_pca.predict(X_test)\n",
    "accuracy_without_pca = accuracy_score(y_test, y_pred_without_pca)\n",
    "print(\"Accuracy without PCA:\", accuracy_without_pca)\n",
    "\n",
    "# Build neural network with PCA\n",
    "X_pca_train, X_pca_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.33, random_state=42)\n",
    "model_with_pca = MLPClassifier(hidden_layer_sizes=(500, 500), activation='relu', random_state=42)\n",
    "model_with_pca.fit(X_pca_train, y_train)\n",
    "y_pred_with_pca = model_with_pca.predict(X_pca_test)\n",
    "accuracy_with_pca = accuracy_score(y_test, y_pred_with_pca)\n",
    "print(\"Accuracy with PCA:\", accuracy_with_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3d9210",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3c64c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (569, 30)\n",
      "First few rows of the dataset:\n",
      "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
      "0        17.99         10.38          122.80     1001.0          0.11840   \n",
      "1        20.57         17.77          132.90     1326.0          0.08474   \n",
      "2        19.69         21.25          130.00     1203.0          0.10960   \n",
      "3        11.42         20.38           77.58      386.1          0.14250   \n",
      "4        20.29         14.34          135.10     1297.0          0.10030   \n",
      "\n",
      "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
      "0           0.27760          0.3001              0.14710         0.2419   \n",
      "1           0.07864          0.0869              0.07017         0.1812   \n",
      "2           0.15990          0.1974              0.12790         0.2069   \n",
      "3           0.28390          0.2414              0.10520         0.2597   \n",
      "4           0.13280          0.1980              0.10430         0.1809   \n",
      "\n",
      "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
      "0                 0.07871  ...         25.38          17.33           184.60   \n",
      "1                 0.05667  ...         24.99          23.41           158.80   \n",
      "2                 0.05999  ...         23.57          25.53           152.50   \n",
      "3                 0.09744  ...         14.91          26.50            98.87   \n",
      "4                 0.05883  ...         22.54          16.67           152.20   \n",
      "\n",
      "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
      "0      2019.0            0.1622             0.6656           0.7119   \n",
      "1      1956.0            0.1238             0.1866           0.2416   \n",
      "2      1709.0            0.1444             0.4245           0.4504   \n",
      "3       567.7            0.2098             0.8663           0.6869   \n",
      "4      1575.0            0.1374             0.2050           0.4000   \n",
      "\n",
      "   worst concave points  worst symmetry  worst fractal dimension  \n",
      "0                0.2654          0.4601                  0.11890  \n",
      "1                0.1860          0.2750                  0.08902  \n",
      "2                0.2430          0.3613                  0.08758  \n",
      "3                0.2575          0.6638                  0.17300  \n",
      "4                0.1625          0.2364                  0.07678  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "Feature Names:\n",
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "Mean of the Transformed Input:\n",
      "[-3.16286735e-15 -6.53060890e-15 -7.07889127e-16 -8.79983452e-16\n",
      "  6.13217737e-15 -1.12036918e-15 -4.42138027e-16  9.73249991e-16\n",
      " -1.97167024e-15 -1.45363120e-15 -9.07641468e-16 -8.85349205e-16\n",
      "  1.77367396e-15 -8.29155139e-16 -7.54180940e-16 -3.92187747e-16\n",
      "  7.91789988e-16 -2.73946068e-16 -3.10823423e-16 -3.36676596e-16\n",
      " -2.33322442e-15  1.76367415e-15 -1.19802625e-15  5.04966114e-16\n",
      " -5.21317026e-15 -2.17478837e-15  6.85645643e-16 -1.41265636e-16\n",
      " -2.28956670e-15  2.57517109e-15]\n",
      "Standard Deviation of the Transformed Input:\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1.]\n",
      "Variance Ratio of the Components:\n",
      "[0.44272026 0.18971182]\n",
      "Epoch 1/20\n",
      "3/3 [==============================] - 1s 67ms/step - loss: 0.5568 - accuracy: 0.7585 - val_loss: 0.3175 - val_accuracy: 0.9415\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.2534 - accuracy: 0.9501 - val_loss: 0.1648 - val_accuracy: 0.9415\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.1468 - accuracy: 0.9554 - val_loss: 0.1078 - val_accuracy: 0.9415\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.1071 - accuracy: 0.9659 - val_loss: 0.0819 - val_accuracy: 0.9521\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0844 - accuracy: 0.9738 - val_loss: 0.0684 - val_accuracy: 0.9734\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0730 - accuracy: 0.9738 - val_loss: 0.0645 - val_accuracy: 0.9734\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0632 - accuracy: 0.9790 - val_loss: 0.0633 - val_accuracy: 0.9681\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.0577 - accuracy: 0.9816 - val_loss: 0.0597 - val_accuracy: 0.9628\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0514 - accuracy: 0.9816 - val_loss: 0.0573 - val_accuracy: 0.9681\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.0464 - accuracy: 0.9816 - val_loss: 0.0545 - val_accuracy: 0.9734\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0408 - accuracy: 0.9843 - val_loss: 0.0537 - val_accuracy: 0.9734\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0377 - accuracy: 0.9869 - val_loss: 0.0557 - val_accuracy: 0.9787\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0334 - accuracy: 0.9895 - val_loss: 0.0584 - val_accuracy: 0.9734\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0299 - accuracy: 0.9895 - val_loss: 0.0606 - val_accuracy: 0.9734\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0268 - accuracy: 0.9948 - val_loss: 0.0608 - val_accuracy: 0.9734\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0241 - accuracy: 0.9948 - val_loss: 0.0611 - val_accuracy: 0.9734\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0218 - accuracy: 0.9948 - val_loss: 0.0601 - val_accuracy: 0.9734\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0188 - accuracy: 0.9948 - val_loss: 0.0596 - val_accuracy: 0.9787\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.0169 - accuracy: 0.9948 - val_loss: 0.0591 - val_accuracy: 0.9787\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0145 - accuracy: 0.9974 - val_loss: 0.0604 - val_accuracy: 0.9734\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0604 - accuracy: 0.9734\n",
      "Accuracy without PCA: 0.9734042286872864\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 455\n  y sizes: 381\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 89>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# With PCA\u001b[39;00m\n\u001b[0;32m     88\u001b[0m model_with_pca\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 89\u001b[0m \u001b[43mmodel_with_pca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_pca\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m _, accuracy_with_pca \u001b[38;5;241m=\u001b[39m model_with_pca\u001b[38;5;241m.\u001b[39mevaluate(pca\u001b[38;5;241m.\u001b[39mtransform(X_test), y_test_encoded)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy with PCA:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_with_pca)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\data_adapter.py:1852\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1845\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m sizes: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1846\u001b[0m         label,\n\u001b[0;32m   1847\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m   1848\u001b[0m             \u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(single_data)\n\u001b[0;32m   1849\u001b[0m         ),\n\u001b[0;32m   1850\u001b[0m     )\n\u001b[0;32m   1851\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1852\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 455\n  y sizes: 381\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Step 1: Load Breast Cancer Dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Step 2: Print shape and first few rows\n",
    "print(\"Dataset Shape:\", data.data.shape)\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(pd.DataFrame(data.data, columns=data.feature_names).head())\n",
    "\n",
    "# Step 3: Define features and target\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Step 4: Print feature names\n",
    "print(\"Feature Names:\")\n",
    "print(data.feature_names)\n",
    "\n",
    "# Step 5: Use StandardScaler() to transform the input\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 6: Print the mean and std of the transformed input\n",
    "print(\"Mean of the Transformed Input:\")\n",
    "print(np.mean(X_scaled, axis=0))\n",
    "print(\"Standard Deviation of the Transformed Input:\")\n",
    "print(np.std(X_scaled, axis=0))\n",
    "\n",
    "# Step 7: Define the number of principal components to 2\n",
    "n_components = 2\n",
    "\n",
    "# Step 8: Fit the input into PCA\n",
    "pca = PCA(n_components=n_components)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Step 9: Print the variance ratio of the components\n",
    "print(\"Variance Ratio of the Components:\")\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "# Step 10: Divide the dataset into training set and testing set (0.33)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Step 11: Build neural networks for dataset without PCA and with PCA\n",
    "input_shape = X_train.shape[1]\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "# Without PCA\n",
    "model_no_pca = keras.Sequential([\n",
    "    layers.Dense(500, activation='relu', input_shape=(input_shape,)),\n",
    "    layers.Dense(500, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# With PCA\n",
    "input_shape_pca = X_pca.shape[1]\n",
    "model_with_pca = keras.Sequential([\n",
    "    layers.Dense(500, activation='relu', input_shape=(input_shape_pca,)),\n",
    "    layers.Dense(500, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Step 12: Compile the models\n",
    "model_no_pca.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_with_pca.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# One-hot encode the target variables\n",
    "encoder = OneHotEncoder(categories='auto')\n",
    "y_train_encoded = encoder.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
    "y_test_encoded = encoder.transform(y_test.reshape(-1, 1)).toarray()\n",
    "\n",
    "# Step 13: Print the accuracy for both networks\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "# Without PCA\n",
    "model_no_pca.fit(X_train, y_train_encoded, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test_encoded))\n",
    "_, accuracy_no_pca = model_no_pca.evaluate(X_test, y_test_encoded)\n",
    "print(\"Accuracy without PCA:\", accuracy_no_pca)\n",
    "\n",
    "# With PCA\n",
    "model_with_pca.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_with_pca.fit(X_pca, y_train_encoded, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "_, accuracy_with_pca = model_with_pca.evaluate(pca.transform(X_test), y_test_encoded)\n",
    "print(\"Accuracy with PCA:\", accuracy_with_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd25befb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea4222aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (569, 30)\n",
      "First few rows of the dataset:\n",
      "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
      "0        17.99         10.38          122.80     1001.0          0.11840   \n",
      "1        20.57         17.77          132.90     1326.0          0.08474   \n",
      "2        19.69         21.25          130.00     1203.0          0.10960   \n",
      "3        11.42         20.38           77.58      386.1          0.14250   \n",
      "4        20.29         14.34          135.10     1297.0          0.10030   \n",
      "\n",
      "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
      "0           0.27760          0.3001              0.14710         0.2419   \n",
      "1           0.07864          0.0869              0.07017         0.1812   \n",
      "2           0.15990          0.1974              0.12790         0.2069   \n",
      "3           0.28390          0.2414              0.10520         0.2597   \n",
      "4           0.13280          0.1980              0.10430         0.1809   \n",
      "\n",
      "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
      "0                 0.07871  ...         25.38          17.33           184.60   \n",
      "1                 0.05667  ...         24.99          23.41           158.80   \n",
      "2                 0.05999  ...         23.57          25.53           152.50   \n",
      "3                 0.09744  ...         14.91          26.50            98.87   \n",
      "4                 0.05883  ...         22.54          16.67           152.20   \n",
      "\n",
      "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
      "0      2019.0            0.1622             0.6656           0.7119   \n",
      "1      1956.0            0.1238             0.1866           0.2416   \n",
      "2      1709.0            0.1444             0.4245           0.4504   \n",
      "3       567.7            0.2098             0.8663           0.6869   \n",
      "4      1575.0            0.1374             0.2050           0.4000   \n",
      "\n",
      "   worst concave points  worst symmetry  worst fractal dimension  \n",
      "0                0.2654          0.4601                  0.11890  \n",
      "1                0.1860          0.2750                  0.08902  \n",
      "2                0.2430          0.3613                  0.08758  \n",
      "3                0.2575          0.6638                  0.17300  \n",
      "4                0.1625          0.2364                  0.07678  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "Feature Names:\n",
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "Mean of the Transformed Input:\n",
      "[-3.16286735e-15 -6.53060890e-15 -7.07889127e-16 -8.79983452e-16\n",
      "  6.13217737e-15 -1.12036918e-15 -4.42138027e-16  9.73249991e-16\n",
      " -1.97167024e-15 -1.45363120e-15 -9.07641468e-16 -8.85349205e-16\n",
      "  1.77367396e-15 -8.29155139e-16 -7.54180940e-16 -3.92187747e-16\n",
      "  7.91789988e-16 -2.73946068e-16 -3.10823423e-16 -3.36676596e-16\n",
      " -2.33322442e-15  1.76367415e-15 -1.19802625e-15  5.04966114e-16\n",
      " -5.21317026e-15 -2.17478837e-15  6.85645643e-16 -1.41265636e-16\n",
      " -2.28956670e-15  2.57517109e-15]\n",
      "Standard Deviation of the Transformed Input:\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1.]\n",
      "Variance Ratio of the Components:\n",
      "[0.44272026 0.18971182]\n",
      "Epoch 1/20\n",
      "3/3 [==============================] - 1s 53ms/step - loss: 0.5615 - accuracy: 0.7480 - val_loss: 0.3135 - val_accuracy: 0.9521\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.2563 - accuracy: 0.9554 - val_loss: 0.1661 - val_accuracy: 0.9468\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.1509 - accuracy: 0.9606 - val_loss: 0.1057 - val_accuracy: 0.9574\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.1115 - accuracy: 0.9685 - val_loss: 0.0785 - val_accuracy: 0.9681\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0857 - accuracy: 0.9764 - val_loss: 0.0657 - val_accuracy: 0.9787\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0705 - accuracy: 0.9764 - val_loss: 0.0633 - val_accuracy: 0.9734\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0629 - accuracy: 0.9816 - val_loss: 0.0627 - val_accuracy: 0.9734\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0594 - accuracy: 0.9790 - val_loss: 0.0615 - val_accuracy: 0.9681\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0527 - accuracy: 0.9790 - val_loss: 0.0583 - val_accuracy: 0.9681\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0475 - accuracy: 0.9843 - val_loss: 0.0524 - val_accuracy: 0.9840\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0423 - accuracy: 0.9869 - val_loss: 0.0514 - val_accuracy: 0.9840\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0377 - accuracy: 0.9921 - val_loss: 0.0532 - val_accuracy: 0.9840\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.0328 - accuracy: 0.9921 - val_loss: 0.0562 - val_accuracy: 0.9734\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0289 - accuracy: 0.9921 - val_loss: 0.0590 - val_accuracy: 0.9734\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.0261 - accuracy: 0.9948 - val_loss: 0.0601 - val_accuracy: 0.9734\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.0243 - accuracy: 0.9948 - val_loss: 0.0587 - val_accuracy: 0.9840\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0207 - accuracy: 0.9921 - val_loss: 0.0560 - val_accuracy: 0.9840\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0182 - accuracy: 0.9921 - val_loss: 0.0549 - val_accuracy: 0.9894\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0164 - accuracy: 0.9921 - val_loss: 0.0547 - val_accuracy: 0.9894\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0143 - accuracy: 0.9948 - val_loss: 0.0568 - val_accuracy: 0.9894\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0568 - accuracy: 0.9894\n",
      "Accuracy without PCA: 0.9893617033958435\n",
      "Epoch 1/20\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.6880 - accuracy: 0.6250"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 188\n  y sizes: 0\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 88>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy without PCA:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_no_pca)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# With PCA\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m \u001b[43mmodel_with_pca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_pca\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.33\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m _, accuracy_with_pca \u001b[38;5;241m=\u001b[39m model_with_pca\u001b[38;5;241m.\u001b[39mevaluate(pca\u001b[38;5;241m.\u001b[39mtransform(X_test), y_test_encoded)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy with PCA:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_with_pca)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\data_adapter.py:1852\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1845\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m sizes: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1846\u001b[0m         label,\n\u001b[0;32m   1847\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m   1848\u001b[0m             \u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(single_data)\n\u001b[0;32m   1849\u001b[0m         ),\n\u001b[0;32m   1850\u001b[0m     )\n\u001b[0;32m   1851\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1852\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 188\n  y sizes: 0\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Step 1: Load Breast Cancer Dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Step 2: Print shape and first few rows\n",
    "print(\"Dataset Shape:\", data.data.shape)\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(pd.DataFrame(data.data, columns=data.feature_names).head())\n",
    "\n",
    "# Step 3: Define features and target\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Step 4: Print feature names\n",
    "print(\"Feature Names:\")\n",
    "print(data.feature_names)\n",
    "\n",
    "# Step 5: Use StandardScaler() to transform the input\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 6: Print the mean and std of the transformed input\n",
    "print(\"Mean of the Transformed Input:\")\n",
    "print(np.mean(X_scaled, axis=0))\n",
    "print(\"Standard Deviation of the Transformed Input:\")\n",
    "print(np.std(X_scaled, axis=0))\n",
    "\n",
    "# Step 7: Define the number of principal components to 2\n",
    "n_components = 2\n",
    "\n",
    "# Step 8: Fit the input into PCA\n",
    "pca = PCA(n_components=n_components)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Step 9: Print the variance ratio of the components\n",
    "print(\"Variance Ratio of the Components:\")\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "# Step 10: Divide the dataset into training set and testing set (0.33)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Step 11: Build neural networks for dataset without PCA and with PCA\n",
    "input_shape = X_train.shape[1]\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "# Without PCA\n",
    "model_no_pca = keras.Sequential([\n",
    "    layers.Dense(500, activation='relu', input_shape=(input_shape,)),\n",
    "    layers.Dense(500, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# With PCA\n",
    "input_shape_pca = X_pca.shape[1]\n",
    "model_with_pca = keras.Sequential([\n",
    "    layers.Dense(500, activation='relu', input_shape=(input_shape_pca,)),\n",
    "    layers.Dense(500, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Step 12: Compile the models\n",
    "model_no_pca.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_with_pca.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# One-hot encode the target variables\n",
    "encoder = OneHotEncoder(categories='auto')\n",
    "y_train_encoded = encoder.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
    "y_test_encoded = encoder.transform(y_test.reshape(-1, 1)).toarray()\n",
    "\n",
    "# Step 13: Print the accuracy for both networks\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "# Without PCA\n",
    "model_no_pca.fit(X_train, y_train_encoded, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test_encoded))\n",
    "_, accuracy_no_pca = model_no_pca.evaluate(X_test, y_test_encoded)\n",
    "print(\"Accuracy without PCA:\", accuracy_no_pca)\n",
    "\n",
    "# With PCA\n",
    "model_with_pca.fit(X_pca, y_train_encoded, batch_size=batch_size, epochs=epochs, validation_split=0.33)\n",
    "_, accuracy_with_pca = model_with_pca.evaluate(pca.transform(X_test), y_test_encoded)\n",
    "print(\"Accuracy with PCA:\", accuracy_with_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09a6942e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (569, 30)\n",
      "First few rows of the dataset:\n",
      "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
      "0        17.99         10.38          122.80     1001.0          0.11840   \n",
      "1        20.57         17.77          132.90     1326.0          0.08474   \n",
      "2        19.69         21.25          130.00     1203.0          0.10960   \n",
      "3        11.42         20.38           77.58      386.1          0.14250   \n",
      "4        20.29         14.34          135.10     1297.0          0.10030   \n",
      "\n",
      "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
      "0           0.27760          0.3001              0.14710         0.2419   \n",
      "1           0.07864          0.0869              0.07017         0.1812   \n",
      "2           0.15990          0.1974              0.12790         0.2069   \n",
      "3           0.28390          0.2414              0.10520         0.2597   \n",
      "4           0.13280          0.1980              0.10430         0.1809   \n",
      "\n",
      "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
      "0                 0.07871  ...         25.38          17.33           184.60   \n",
      "1                 0.05667  ...         24.99          23.41           158.80   \n",
      "2                 0.05999  ...         23.57          25.53           152.50   \n",
      "3                 0.09744  ...         14.91          26.50            98.87   \n",
      "4                 0.05883  ...         22.54          16.67           152.20   \n",
      "\n",
      "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
      "0      2019.0            0.1622             0.6656           0.7119   \n",
      "1      1956.0            0.1238             0.1866           0.2416   \n",
      "2      1709.0            0.1444             0.4245           0.4504   \n",
      "3       567.7            0.2098             0.8663           0.6869   \n",
      "4      1575.0            0.1374             0.2050           0.4000   \n",
      "\n",
      "   worst concave points  worst symmetry  worst fractal dimension  \n",
      "0                0.2654          0.4601                  0.11890  \n",
      "1                0.1860          0.2750                  0.08902  \n",
      "2                0.2430          0.3613                  0.08758  \n",
      "3                0.2575          0.6638                  0.17300  \n",
      "4                0.1625          0.2364                  0.07678  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "Feature Names:\n",
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "Mean of the Transformed Input:\n",
      "[-3.16286735e-15 -6.53060890e-15 -7.07889127e-16 -8.79983452e-16\n",
      "  6.13217737e-15 -1.12036918e-15 -4.42138027e-16  9.73249991e-16\n",
      " -1.97167024e-15 -1.45363120e-15 -9.07641468e-16 -8.85349205e-16\n",
      "  1.77367396e-15 -8.29155139e-16 -7.54180940e-16 -3.92187747e-16\n",
      "  7.91789988e-16 -2.73946068e-16 -3.10823423e-16 -3.36676596e-16\n",
      " -2.33322442e-15  1.76367415e-15 -1.19802625e-15  5.04966114e-16\n",
      " -5.21317026e-15 -2.17478837e-15  6.85645643e-16 -1.41265636e-16\n",
      " -2.28956670e-15  2.57517109e-15]\n",
      "Standard Deviation of the Transformed Input:\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1.]\n",
      "Variance Ratio of the Components:\n",
      "[0.44272026 0.18971182]\n",
      "Epoch 1/20\n",
      "3/3 [==============================] - 1s 54ms/step - loss: 0.4360 - accuracy: 0.9344 - val_loss: 0.1520 - val_accuracy: 1.0000\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0947 - accuracy: 1.0000 - val_loss: 0.0341 - val_accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0199 - accuracy: 1.0000 - val_loss: 0.0090 - val_accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 6.9599e-04 - accuracy: 1.0000 - val_loss: 7.3355e-04 - val_accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.5323e-04 - accuracy: 1.0000 - val_loss: 4.5197e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 2.1780e-04 - accuracy: 1.0000 - val_loss: 3.1078e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.3787e-04 - accuracy: 1.0000 - val_loss: 2.3278e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 1.0445e-04 - accuracy: 1.0000 - val_loss: 1.8614e-04 - val_accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 8.2694e-05 - accuracy: 1.0000 - val_loss: 1.5662e-04 - val_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 6.6857e-05 - accuracy: 1.0000 - val_loss: 1.3712e-04 - val_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 5.8634e-05 - accuracy: 1.0000 - val_loss: 1.2372e-04 - val_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 5.2453e-05 - accuracy: 1.0000 - val_loss: 1.1428e-04 - val_accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 4.8467e-05 - accuracy: 1.0000 - val_loss: 1.0744e-04 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 4.5047e-05 - accuracy: 1.0000 - val_loss: 1.0241e-04 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 4.2787e-05 - accuracy: 1.0000 - val_loss: 9.8636e-05 - val_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 4.1098e-05 - accuracy: 1.0000 - val_loss: 9.5741e-05 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 3.9595e-05 - accuracy: 1.0000 - val_loss: 9.3495e-05 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 3.8446e-05 - accuracy: 1.0000 - val_loss: 9.1721e-05 - val_accuracy: 1.0000\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 9.1721e-05 - accuracy: 1.0000\n",
      "Accuracy without PCA: 1.0\n",
      "Epoch 1/20\n",
      "1/3 [=========>....................] - ETA: 1s - loss: 0.6635 - accuracy: 0.8438"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 188\n  y sizes: 0\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 89>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy without PCA:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_no_pca)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# With PCA\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m \u001b[43mmodel_with_pca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_pca\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.33\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m _, accuracy_with_pca \u001b[38;5;241m=\u001b[39m model_with_pca\u001b[38;5;241m.\u001b[39mevaluate(pca\u001b[38;5;241m.\u001b[39mtransform(X_test), y_test_encoded)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy with PCA:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_with_pca)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\data_adapter.py:1852\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1845\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m sizes: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1846\u001b[0m         label,\n\u001b[0;32m   1847\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m   1848\u001b[0m             \u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(single_data)\n\u001b[0;32m   1849\u001b[0m         ),\n\u001b[0;32m   1850\u001b[0m     )\n\u001b[0;32m   1851\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1852\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 188\n  y sizes: 0\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Step 1: Load Breast Cancer Dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Step 2: Print shape and first few rows\n",
    "print(\"Dataset Shape:\", data.data.shape)\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(pd.DataFrame(data.data, columns=data.feature_names).head())\n",
    "\n",
    "# Step 3: Define features and target\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Step 4: Print feature names\n",
    "print(\"Feature Names:\")\n",
    "print(data.feature_names)\n",
    "\n",
    "# Step 5: Use StandardScaler() to transform the input\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 6: Print the mean and std of the transformed input\n",
    "print(\"Mean of the Transformed Input:\")\n",
    "print(np.mean(X_scaled, axis=0))\n",
    "print(\"Standard Deviation of the Transformed Input:\")\n",
    "print(np.std(X_scaled, axis=0))\n",
    "\n",
    "# Step 7: Define the number of principal components to 2\n",
    "n_components = 2\n",
    "\n",
    "# Step 8: Fit the input into PCA\n",
    "pca = PCA(n_components=n_components)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Step 9: Print the variance ratio of the components\n",
    "print(\"Variance Ratio of the Components:\")\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "# Step 10: Divide the dataset into training set and testing set (0.33)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Step 11: Build neural networks for dataset without PCA and with PCA\n",
    "input_shape = X_train.shape[1]\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "# Without PCA\n",
    "model_no_pca = keras.Sequential([\n",
    "    layers.Dense(500, activation='relu', input_shape=(input_shape,)),\n",
    "    layers.Dense(500, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# With PCA\n",
    "input_shape_pca = X_pca.shape[1]\n",
    "model_with_pca = keras.Sequential([\n",
    "    layers.Dense(500, activation='relu', input_shape=(input_shape_pca,)),\n",
    "    layers.Dense(500, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Step 12: Compile the models\n",
    "model_no_pca.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_with_pca.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# One-hot encode the target variables\n",
    "encoder = OneHotEncoder(categories='auto')\n",
    "y_encoded = encoder.fit_transform(y.reshape(-1, 1)).toarray()\n",
    "y_train_encoded = y_encoded[y_train]\n",
    "y_test_encoded = y_encoded[y_test]\n",
    "\n",
    "# Step 13: Print the accuracy for both networks\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "# Without PCA\n",
    "model_no_pca.fit(X_train, y_train_encoded, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test_encoded))\n",
    "_, accuracy_no_pca = model_no_pca.evaluate(X_test, y_test_encoded)\n",
    "print(\"Accuracy without PCA:\", accuracy_no_pca)\n",
    "\n",
    "# With PCA\n",
    "model_with_pca.fit(X_pca, y_train_encoded, batch_size=batch_size, epochs=epochs, validation_split=0.33)\n",
    "_, accuracy_with_pca = model_with_pca.evaluate(pca.transform(X_test), y_test_encoded)\n",
    "print(\"Accuracy with PCA:\", accuracy_with_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97aab06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f4524d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (569, 30)\n",
      "First few rows of the dataset:\n",
      "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
      "0        17.99         10.38          122.80     1001.0          0.11840   \n",
      "1        20.57         17.77          132.90     1326.0          0.08474   \n",
      "2        19.69         21.25          130.00     1203.0          0.10960   \n",
      "3        11.42         20.38           77.58      386.1          0.14250   \n",
      "4        20.29         14.34          135.10     1297.0          0.10030   \n",
      "\n",
      "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
      "0           0.27760          0.3001              0.14710         0.2419   \n",
      "1           0.07864          0.0869              0.07017         0.1812   \n",
      "2           0.15990          0.1974              0.12790         0.2069   \n",
      "3           0.28390          0.2414              0.10520         0.2597   \n",
      "4           0.13280          0.1980              0.10430         0.1809   \n",
      "\n",
      "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
      "0                 0.07871  ...         25.38          17.33           184.60   \n",
      "1                 0.05667  ...         24.99          23.41           158.80   \n",
      "2                 0.05999  ...         23.57          25.53           152.50   \n",
      "3                 0.09744  ...         14.91          26.50            98.87   \n",
      "4                 0.05883  ...         22.54          16.67           152.20   \n",
      "\n",
      "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
      "0      2019.0            0.1622             0.6656           0.7119   \n",
      "1      1956.0            0.1238             0.1866           0.2416   \n",
      "2      1709.0            0.1444             0.4245           0.4504   \n",
      "3       567.7            0.2098             0.8663           0.6869   \n",
      "4      1575.0            0.1374             0.2050           0.4000   \n",
      "\n",
      "   worst concave points  worst symmetry  worst fractal dimension  \n",
      "0                0.2654          0.4601                  0.11890  \n",
      "1                0.1860          0.2750                  0.08902  \n",
      "2                0.2430          0.3613                  0.08758  \n",
      "3                0.2575          0.6638                  0.17300  \n",
      "4                0.1625          0.2364                  0.07678  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "Feature Names:\n",
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "Mean of the Transformed Input:\n",
      "[-3.16286735e-15 -6.53060890e-15 -7.07889127e-16 -8.79983452e-16\n",
      "  6.13217737e-15 -1.12036918e-15 -4.42138027e-16  9.73249991e-16\n",
      " -1.97167024e-15 -1.45363120e-15 -9.07641468e-16 -8.85349205e-16\n",
      "  1.77367396e-15 -8.29155139e-16 -7.54180940e-16 -3.92187747e-16\n",
      "  7.91789988e-16 -2.73946068e-16 -3.10823423e-16 -3.36676596e-16\n",
      " -2.33322442e-15  1.76367415e-15 -1.19802625e-15  5.04966114e-16\n",
      " -5.21317026e-15 -2.17478837e-15  6.85645643e-16 -1.41265636e-16\n",
      " -2.28956670e-15  2.57517109e-15]\n",
      "Standard Deviation of the Transformed Input:\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1.]\n",
      "Variance Ratio of the Components:\n",
      "[0.44272026 0.18971182]\n",
      "Epoch 1/20\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.5367 - accuracy: 0.7900 - val_loss: 0.3066 - val_accuracy: 0.9468\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.2540 - accuracy: 0.9554 - val_loss: 0.1604 - val_accuracy: 0.9468\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.1457 - accuracy: 0.9580 - val_loss: 0.1034 - val_accuracy: 0.9521\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.1054 - accuracy: 0.9711 - val_loss: 0.0771 - val_accuracy: 0.9734\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0839 - accuracy: 0.9764 - val_loss: 0.0643 - val_accuracy: 0.9787\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0727 - accuracy: 0.9790 - val_loss: 0.0645 - val_accuracy: 0.9734\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0640 - accuracy: 0.9790 - val_loss: 0.0637 - val_accuracy: 0.9681\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0596 - accuracy: 0.9790 - val_loss: 0.0614 - val_accuracy: 0.9734\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0521 - accuracy: 0.9816 - val_loss: 0.0547 - val_accuracy: 0.9787\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0455 - accuracy: 0.9843 - val_loss: 0.0509 - val_accuracy: 0.9894\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0413 - accuracy: 0.9869 - val_loss: 0.0517 - val_accuracy: 0.9840\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0364 - accuracy: 0.9895 - val_loss: 0.0522 - val_accuracy: 0.9787\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0326 - accuracy: 0.9921 - val_loss: 0.0544 - val_accuracy: 0.9787\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0288 - accuracy: 0.9921 - val_loss: 0.0558 - val_accuracy: 0.9787\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0262 - accuracy: 0.9948 - val_loss: 0.0564 - val_accuracy: 0.9787\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0231 - accuracy: 0.9948 - val_loss: 0.0558 - val_accuracy: 0.9787\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0207 - accuracy: 0.9948 - val_loss: 0.0553 - val_accuracy: 0.9787\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0188 - accuracy: 0.9948 - val_loss: 0.0539 - val_accuracy: 0.9840\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0162 - accuracy: 0.9974 - val_loss: 0.0543 - val_accuracy: 0.9894\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0140 - accuracy: 0.9974 - val_loss: 0.0545 - val_accuracy: 0.9840\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0545 - accuracy: 0.9840\n",
      "Accuracy without PCA: 0.9840425252914429\n",
      "Epoch 1/20\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.6954 - accuracy: 0.5391"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 188\n  y sizes: 0\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 88>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy without PCA:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_no_pca)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# With PCA\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m \u001b[43mmodel_with_pca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_pca\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_categorical\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.33\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m _, accuracy_with_pca \u001b[38;5;241m=\u001b[39m model_with_pca\u001b[38;5;241m.\u001b[39mevaluate(pca\u001b[38;5;241m.\u001b[39mtransform(X_test), y_test_categorical)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy with PCA:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_with_pca)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\data_adapter.py:1852\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1845\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m sizes: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1846\u001b[0m         label,\n\u001b[0;32m   1847\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m   1848\u001b[0m             \u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(single_data)\n\u001b[0;32m   1849\u001b[0m         ),\n\u001b[0;32m   1850\u001b[0m     )\n\u001b[0;32m   1851\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1852\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 188\n  y sizes: 0\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Step 1: Load Breast Cancer Dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Step 2: Print shape and first few rows\n",
    "print(\"Dataset Shape:\", data.data.shape)\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(pd.DataFrame(data.data, columns=data.feature_names).head())\n",
    "\n",
    "# Step 3: Define features and target\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Step 4: Print feature names\n",
    "print(\"Feature Names:\")\n",
    "print(data.feature_names)\n",
    "\n",
    "# Step 5: Use StandardScaler() to transform the input\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 6: Print the mean and std of the transformed input\n",
    "print(\"Mean of the Transformed Input:\")\n",
    "print(np.mean(X_scaled, axis=0))\n",
    "print(\"Standard Deviation of the Transformed Input:\")\n",
    "print(np.std(X_scaled, axis=0))\n",
    "\n",
    "# Step 7: Define the number of principal components to 2\n",
    "n_components = 2\n",
    "\n",
    "# Step 8: Fit the input into PCA\n",
    "pca = PCA(n_components=n_components)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Step 9: Print the variance ratio of the components\n",
    "print(\"Variance Ratio of the Components:\")\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "# Step 10: Divide the dataset into training set and testing set (0.33)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Step 11: Build neural networks for dataset without PCA and with PCA\n",
    "input_shape = X_train.shape[1]\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "# Without PCA\n",
    "model_no_pca = keras.Sequential([\n",
    "    layers.Dense(500, activation='relu', input_shape=(input_shape,)),\n",
    "    layers.Dense(500, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# With PCA\n",
    "input_shape_pca = X_pca.shape[1]\n",
    "model_with_pca = keras.Sequential([\n",
    "    layers.Dense(500, activation='relu', input_shape=(input_shape_pca,)),\n",
    "    layers.Dense(500, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Step 12: Compile the models\n",
    "model_no_pca.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_with_pca.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Convert the target variables to categorical\n",
    "y_train_categorical = to_categorical(y_train, num_classes)\n",
    "y_test_categorical = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Step 13: Print the accuracy for both networks\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "# Without PCA\n",
    "model_no_pca.fit(X_train, y_train_categorical, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test_categorical))\n",
    "_, accuracy_no_pca = model_no_pca.evaluate(X_test, y_test_categorical)\n",
    "print(\"Accuracy without PCA:\", accuracy_no_pca)\n",
    "\n",
    "# With PCA\n",
    "model_with_pca.fit(X_pca, y_train_categorical, batch_size=batch_size, epochs=epochs, validation_split=0.33)\n",
    "_, accuracy_with_pca = model_with_pca.evaluate(pca.transform(X_test), y_test_categorical)\n",
    "print(\"Accuracy with PCA:\", accuracy_with_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9462987d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83aca824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (569, 30)\n",
      "First few rows of the dataset:\n",
      "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
      "0        17.99         10.38          122.80     1001.0          0.11840   \n",
      "1        20.57         17.77          132.90     1326.0          0.08474   \n",
      "2        19.69         21.25          130.00     1203.0          0.10960   \n",
      "3        11.42         20.38           77.58      386.1          0.14250   \n",
      "4        20.29         14.34          135.10     1297.0          0.10030   \n",
      "\n",
      "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
      "0           0.27760          0.3001              0.14710         0.2419   \n",
      "1           0.07864          0.0869              0.07017         0.1812   \n",
      "2           0.15990          0.1974              0.12790         0.2069   \n",
      "3           0.28390          0.2414              0.10520         0.2597   \n",
      "4           0.13280          0.1980              0.10430         0.1809   \n",
      "\n",
      "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
      "0                 0.07871  ...         25.38          17.33           184.60   \n",
      "1                 0.05667  ...         24.99          23.41           158.80   \n",
      "2                 0.05999  ...         23.57          25.53           152.50   \n",
      "3                 0.09744  ...         14.91          26.50            98.87   \n",
      "4                 0.05883  ...         22.54          16.67           152.20   \n",
      "\n",
      "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
      "0      2019.0            0.1622             0.6656           0.7119   \n",
      "1      1956.0            0.1238             0.1866           0.2416   \n",
      "2      1709.0            0.1444             0.4245           0.4504   \n",
      "3       567.7            0.2098             0.8663           0.6869   \n",
      "4      1575.0            0.1374             0.2050           0.4000   \n",
      "\n",
      "   worst concave points  worst symmetry  worst fractal dimension  \n",
      "0                0.2654          0.4601                  0.11890  \n",
      "1                0.1860          0.2750                  0.08902  \n",
      "2                0.2430          0.3613                  0.08758  \n",
      "3                0.2575          0.6638                  0.17300  \n",
      "4                0.1625          0.2364                  0.07678  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "Feature Names:\n",
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "Mean of the Transformed Input:\n",
      "[-3.16286735e-15 -6.53060890e-15 -7.07889127e-16 -8.79983452e-16\n",
      "  6.13217737e-15 -1.12036918e-15 -4.42138027e-16  9.73249991e-16\n",
      " -1.97167024e-15 -1.45363120e-15 -9.07641468e-16 -8.85349205e-16\n",
      "  1.77367396e-15 -8.29155139e-16 -7.54180940e-16 -3.92187747e-16\n",
      "  7.91789988e-16 -2.73946068e-16 -3.10823423e-16 -3.36676596e-16\n",
      " -2.33322442e-15  1.76367415e-15 -1.19802625e-15  5.04966114e-16\n",
      " -5.21317026e-15 -2.17478837e-15  6.85645643e-16 -1.41265636e-16\n",
      " -2.28956670e-15  2.57517109e-15]\n",
      "Standard Deviation of the Transformed Input:\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1.]\n",
      "Variance Ratio of the Components:\n",
      "[0.44272026 0.18971182]\n",
      "Epoch 1/20\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.5365 - accuracy: 0.7953 - val_loss: 0.3043 - val_accuracy: 0.9415\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.2466 - accuracy: 0.9528 - val_loss: 0.1623 - val_accuracy: 0.9468\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.1471 - accuracy: 0.9554 - val_loss: 0.1088 - val_accuracy: 0.9415\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.1073 - accuracy: 0.9633 - val_loss: 0.0813 - val_accuracy: 0.9628\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0853 - accuracy: 0.9711 - val_loss: 0.0685 - val_accuracy: 0.9734\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0729 - accuracy: 0.9738 - val_loss: 0.0663 - val_accuracy: 0.9734\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0639 - accuracy: 0.9790 - val_loss: 0.0648 - val_accuracy: 0.9628\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0580 - accuracy: 0.9816 - val_loss: 0.0598 - val_accuracy: 0.9681\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.0518 - accuracy: 0.9816 - val_loss: 0.0542 - val_accuracy: 0.9787\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.0456 - accuracy: 0.9843 - val_loss: 0.0540 - val_accuracy: 0.9734\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0412 - accuracy: 0.9843 - val_loss: 0.0556 - val_accuracy: 0.9734\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0372 - accuracy: 0.9843 - val_loss: 0.0587 - val_accuracy: 0.9734\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.0328 - accuracy: 0.9843 - val_loss: 0.0608 - val_accuracy: 0.9787\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0302 - accuracy: 0.9895 - val_loss: 0.0632 - val_accuracy: 0.9734\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0269 - accuracy: 0.9921 - val_loss: 0.0638 - val_accuracy: 0.9681\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0236 - accuracy: 0.9948 - val_loss: 0.0619 - val_accuracy: 0.9734\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0215 - accuracy: 0.9921 - val_loss: 0.0590 - val_accuracy: 0.9734\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0186 - accuracy: 0.9948 - val_loss: 0.0587 - val_accuracy: 0.9787\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0172 - accuracy: 0.9948 - val_loss: 0.0617 - val_accuracy: 0.9787\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0144 - accuracy: 0.9974 - val_loss: 0.0619 - val_accuracy: 0.9840\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0619 - accuracy: 0.9840\n",
      "Accuracy without PCA: 0.9840425252914429\n",
      "Epoch 1/20\n",
      "3/3 [==============================] - 1s 52ms/step - loss: 0.5632 - accuracy: 0.8189 - val_loss: 0.3520 - val_accuracy: 0.9309\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3038 - accuracy: 0.9370 - val_loss: 0.2173 - val_accuracy: 0.9362\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.2025 - accuracy: 0.9449 - val_loss: 0.1503 - val_accuracy: 0.9468\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.1624 - accuracy: 0.9475 - val_loss: 0.1191 - val_accuracy: 0.9521\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.1445 - accuracy: 0.9449 - val_loss: 0.1031 - val_accuracy: 0.9628\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.1404 - accuracy: 0.9475 - val_loss: 0.0955 - val_accuracy: 0.9681\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.1387 - accuracy: 0.9501 - val_loss: 0.0919 - val_accuracy: 0.9734\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.1400 - accuracy: 0.9475 - val_loss: 0.0881 - val_accuracy: 0.9734\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.1414 - accuracy: 0.9475 - val_loss: 0.0904 - val_accuracy: 0.9734\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.1405 - accuracy: 0.9475 - val_loss: 0.0883 - val_accuracy: 0.9734\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.1395 - accuracy: 0.9528 - val_loss: 0.0874 - val_accuracy: 0.9628\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.1389 - accuracy: 0.9501 - val_loss: 0.0890 - val_accuracy: 0.9628\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 15ms/step - loss: 0.1374 - accuracy: 0.9528 - val_loss: 0.0938 - val_accuracy: 0.9628\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.1367 - accuracy: 0.9501 - val_loss: 0.0952 - val_accuracy: 0.9628\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.1356 - accuracy: 0.9501 - val_loss: 0.0944 - val_accuracy: 0.9628\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.1346 - accuracy: 0.9501 - val_loss: 0.0953 - val_accuracy: 0.9628\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.1346 - accuracy: 0.9528 - val_loss: 0.0960 - val_accuracy: 0.9628\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.1336 - accuracy: 0.9528 - val_loss: 0.0946 - val_accuracy: 0.9628\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.1339 - accuracy: 0.9501 - val_loss: 0.0978 - val_accuracy: 0.9628\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.1332 - accuracy: 0.9501 - val_loss: 0.1025 - val_accuracy: 0.9628\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1025 - accuracy: 0.9628\n",
      "Accuracy with PCA: 0.9627659320831299\n",
      "Accuracy without PCA: 0.9840425252914429\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Step 1: Load Breast Cancer Dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Step 2: Print shape and first few rows\n",
    "print(\"Dataset Shape:\", data.data.shape)\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(pd.DataFrame(data.data, columns=data.feature_names).head())\n",
    "\n",
    "# Step 3: Define features and target\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Step 4: Print feature names\n",
    "print(\"Feature Names:\")\n",
    "print(data.feature_names)\n",
    "\n",
    "# Step 5: Use StandardScaler() to transform the input\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 6: Print the mean and std of the transformed input\n",
    "print(\"Mean of the Transformed Input:\")\n",
    "print(np.mean(X_scaled, axis=0))\n",
    "print(\"Standard Deviation of the Transformed Input:\")\n",
    "print(np.std(X_scaled, axis=0))\n",
    "\n",
    "# Step 7: Define the number of principal components to 2\n",
    "n_components = 2\n",
    "\n",
    "# Step 8: Fit the input into PCA\n",
    "pca = PCA(n_components=n_components)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Step 9: Print the variance ratio of the components\n",
    "print(\"Variance Ratio of the Components:\")\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "# Step 10: Divide the dataset into training set and testing set (0.33)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Step 11: Build neural networks for dataset without PCA and with PCA\n",
    "input_shape = X_train.shape[1]\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "# Without PCA\n",
    "model_no_pca = keras.Sequential([\n",
    "    layers.Dense(500, activation='relu', input_shape=(input_shape,)),\n",
    "    layers.Dense(500, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# With PCA\n",
    "input_shape_pca = X_pca.shape[1]\n",
    "model_with_pca = keras.Sequential([\n",
    "    layers.Dense(500, activation='relu', input_shape=(input_shape_pca,)),\n",
    "    layers.Dense(500, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Step 12: Compile the models\n",
    "model_no_pca.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_with_pca.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Convert the target variables to categorical\n",
    "y_train_categorical = to_categorical(y_train, num_classes)\n",
    "y_test_categorical = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Step 13: Print the accuracy for both networks\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "# Without PCA\n",
    "model_no_pca.fit(X_train, y_train_categorical, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test_categorical))\n",
    "_, accuracy_no_pca = model_no_pca.evaluate(X_test, y_test_categorical)\n",
    "print(\"Accuracy without PCA:\", accuracy_no_pca)\n",
    "\n",
    "# With PCA\n",
    "X_train_pca, X_test_pca, _, _ = train_test_split(X_pca, y, test_size=0.33, random_state=42)\n",
    "model_with_pca.fit(X_train_pca, y_train_categorical, batch_size=batch_size, epochs=epochs, validation_data=(X_test_pca, y_test_categorical))\n",
    "_, accuracy_with_pca = model_with_pca.evaluate(X_test_pca, y_test_categorical)\n",
    "print(\"Accuracy with PCA:\", accuracy_with_pca)\n",
    "\n",
    "print(\"Accuracy without PCA:\", accuracy_no_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b901c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55628aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b632ae47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
